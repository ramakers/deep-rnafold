{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/software/tensorflow/1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "\n",
    "from six.moves import cPickle\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "from simrna_drifts import *\n",
    "from tensorflow.python.client import device_lib\n",
    "import os    \n",
    "import rna_enc_pdb\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -ltr /gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/vqvae100_3d_8/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cp /gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/vqvae100_3d_8/ckpt/plots/*loss* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5345\n",
      "8\n",
      "8\n",
      "Done\n",
      "device_list: ['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'] \n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-1ec38e81e3c0>:690: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620563f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620563f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620563f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620563f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "1 (?, 100, 100, 32)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210cada0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210cada0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210cada0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210cada0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "2 (?, 100, 100, 16)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96204abd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96204abd30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96204abd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96204abd30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "3 (?, 100, 100, 8)\n",
      "WARNING:tensorflow:From <ipython-input-4-1ec38e81e3c0>:815: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x2b96204ab780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x2b96204ab780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x2b96204ab780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x2b96204ab780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210f6400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210f6400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210f6400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210f6400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x2b96210ca7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x2b96210ca7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x2b96210ca7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x2b96210ca7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210f6630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210f6630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210f6630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210f6630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "4 (?, 100, 100, 8)\n",
      "5 (?, 100, 100, 8)\n",
      "----------\n",
      "WARNING:tensorflow:From /home/ramakers/.local/lib/python3.6/site-packages/sonnet/python/modules/nets/vqvae.py:179: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9621155048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9621155048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9621155048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9621155048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213530f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213530f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213530f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213530f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "1 (?, 100, 100, 8)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "2 (?, 100, 100, 8)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96213f0438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "3 (?, 100, 100, 16)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ee4978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ee4978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ee4978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ee4978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b962141a358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b962141a358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b962141a358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b962141a358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "4 (?, 100, 100, 16)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ca6b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ca6b00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ca6b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ca6b00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "5 (?, 100, 100, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ceee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ceee48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ceee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ceee48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "5 (?, 100, 100, 64)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620d71f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620d71f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620d71f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620d71f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620d71a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620d71a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620d71a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620d71a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "5 (?, 100, 100, 64)\n",
      "6 (?, 100, 100, 64)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ceeeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ceeeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ceeeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620ceeeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620dd8400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620dd8400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620dd8400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620dd8400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228c0f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228c0f60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228c0f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228c0f60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "1 (?, 100, 100, 8)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "2 (?, 100, 100, 8)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4b00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96228e4b00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "3 (?, 100, 100, 16)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96216552b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96216552b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96216552b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96216552b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9621600898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9621600898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9621600898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9621600898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "4 (?, 100, 100, 16)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96216046a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96216046a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96216046a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96216046a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "5 (?, 100, 100, 32)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96215fd0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96215fd0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96215fd0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96215fd0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "5 (?, 100, 100, 64)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620e1fa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620e1fa90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620e1fa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b9620e1fa90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96229184a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96229184a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96229184a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96229184a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "5 (?, 100, 100, 64)\n",
      "6 (?, 100, 100, 64)\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210d7438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210d7438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210d7438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b96210d7438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b962165b320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b962165b320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b962165b320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x2b962165b320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Done setting up graph.\n",
      "50\n",
      "50\n",
      "ckpt dir:  /sopran/shared/embeddings/vqvae_3d_100_0707/\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "enc_seq_pdb   = rna_enc_pdb.RNAEncoder(L=100, args=\"seq\")\n",
    "\n",
    "ids_puzzles = ['3mei', '3p59', '3owz', '3v7e', '4p9r', '4gxy', '4r4v', '4l81', '5kpy', \n",
    "           '4lck', '5lys', '5lyu', '5lyv', '5iem', '4qlm', '4xw7', '5ddo', '5di4', \n",
    "           '5k7c', '5tpy', '5t5a', '5y85', '5y87', '5nwq', '5nz6', '6jq5', '6jq6',\n",
    "           '6e8u', '6ol3', '6p2h', '6pmo', '6pom', '6ufm']\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'#8,9'\n",
    "\n",
    "##### SET NEW MAXIMAL SEQ LENGTH !!! #####\n",
    "PARAMS               = {}\n",
    "PARAMS['batch_size'] = 1\n",
    "PARAMS['RNA_len']    = max_len = 100\n",
    "PARAMS['max_depth']  = 64 # 8*8 atom level\n",
    "PARAMS['N_GPUS'] = 1\n",
    "\n",
    "# Set hyper-parameters.\n",
    "batch_size = PARAMS['batch_size']\n",
    "image_size = PARAMS['RNA_len'] \n",
    "\n",
    "# 100k steps should take < 30 minutes on a modern (>= 2017) GPU.\n",
    "num_training_updates = 50 \n",
    "\n",
    "# 2901 ckpt\n",
    "num_hiddens = 64 \n",
    "num_residual_hiddens = 12#32\n",
    "num_residual_layers = 12#12\n",
    "\n",
    "num_hiddens = 16\n",
    "num_residual_hiddens = 4\n",
    "num_residual_layers = 4\n",
    "\n",
    "\n",
    "#01042020 \n",
    "embedding_dim = 8   #PARAMS['max_depth']\n",
    "#embedding_dim = 12   #PARAMS['max_depth']\n",
    "# The higher this value, the higher the capacity in the information bottleneck.\n",
    "num_embeddings = 3\n",
    "sub_str = '_L100_lre5_emb'+str(num_embeddings)+'_dim'+str(embedding_dim)\n",
    "\n",
    "head_ckpt_dir = '/gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/01022022/ckpt/'\n",
    "head_sopran_dir = '/gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/01022022/'\n",
    "head_ckpt_dir = '/gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/01022022/ckpt/'\n",
    "head_ckpt_dir = '/sopran/shared/embeddings/vqvae_3d_100_0707/'\n",
    "\n",
    "head_sopran_dir = '/gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/vqvae100_3d_8/'\n",
    "\n",
    "with open( '/gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/L64x64_substructure_23022022.pkl', 'rb') as f:\n",
    "    traindata_df, valdata_df, testdata_df = pickle.load( f)\n",
    "print(len(traindata_df))\n",
    "\n",
    "\n",
    "\n",
    "atom_list = [\"P\",\"C4'\",\"C2-GA_Pur\",\"C6\",\"N9\",\"C2-CU_Pyr\",\"C4\",\"N1\"]\n",
    "atid = dict(zip(atom_list, np.arange(len(atom_list)))) # atom to index dict\n",
    "NUM_ATOMS = len(atom_list)\n",
    "print(NUM_ATOMS)\n",
    "\n",
    "def map_idx(i):\n",
    "    \"\"\"\n",
    "    developed in /alto/chris/projects/main/rna_3d/reconstruct_3d_from_distances/20200728_reverse_symmetrization.ipynb\n",
    "    \"\"\"\n",
    "    within_index  = i % NUM_ATOMS\n",
    "    between_index = i//NUM_ATOMS\n",
    "    return within_index*NUM_ATOMS + between_index\n",
    "\n",
    "symmetrization_indices = map_idx(np.arange(NUM_ATOMS**2))\n",
    "\n",
    "def symmetrize_LxLx64(D):\n",
    "    \"\"\"\n",
    "    developed in /alto/chris/projects/main/rna_3d/reconstruct_3d_from_distances/20200728_reverse_symmetrization.ipynb\n",
    "    input:  D [L,L,64]\n",
    "    output: D [L,L,64]\n",
    "    \n",
    "    If this function is applied to an unsymmetrized tensor, it will be symmetrized.\n",
    "    If this function is applied to a symmetrized tensor, it will be un-symmetrized!\n",
    "    \n",
    "    \"\"\"    \n",
    "    upper = np.transpose(np.triu(np.transpose(D,[2,0,1]), 0),[1,2,0])  # 0 includes diagonal\n",
    "    lower = np.transpose(np.tril(np.transpose(D,[2,0,1]), -1),[1,2,0]) # 0 includes diagonal\n",
    "    return upper + lower[:,:,symmetrization_indices]\n",
    "\n",
    "\n",
    "def Lx3_coords_to_LxLx64_tensor(array_a, *args):\n",
    "\n",
    "    \"\"\"\n",
    "    !!! THIS FUNCTION WAS FORMERLY CALLED get_3D_tensor !!!\n",
    "    \n",
    "    originally developed in:\n",
    "    /export/chrisblum/projects/main/rna_3d/data_loaders_and_conversion/20200713_develop_data_augmentation_data_loader.ipynb\n",
    "    and modified in:\n",
    "    /alto/chris/projects/main/rna_3d/reconstruct_3d_from_distances/20200728_LL64_to_L3_Julius_reconstructions_faster_mirrorfix_symmetrizationfix.ipynb    \n",
    "\n",
    "    input(s):\n",
    "        either \n",
    "            coords [Ra*8, 3] and optionally [Rb*8, 3]\n",
    "        or\n",
    "            mask [Ra*8] and optionally [Rb*8]\n",
    "        \n",
    "    output:\n",
    "        either\n",
    "            D [Ra, Ra, 64]\n",
    "        or\n",
    "            D [Ra, Rb, 64]\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if len(args)==1:\n",
    "        array_b = args[0]\n",
    "    else:\n",
    "        array_b = array_a\n",
    "    \n",
    "    array_a_shape = array_a.shape\n",
    "    array_b_shape = array_b.shape\n",
    "    if array_a_shape[0]%NUM_ATOMS!=0:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_a_shape)\n",
    "    if array_b_shape[0]%NUM_ATOMS!=0:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_b_shape)        \n",
    "    \n",
    "    if len(array_a_shape)!=len(array_b_shape):\n",
    "        raise Exception('Inputs have different dimensions:', array_a_shape, array_b_shape)  \n",
    "    \n",
    "    ### assume that input was \"coords\"\n",
    "    if len(array_a_shape)==2:\n",
    "        Ra = len(array_a)//NUM_ATOMS\n",
    "        Rb = len(array_b)//NUM_ATOMS\n",
    "        A = array_a.reshape([Ra, 1, NUM_ATOMS, 1, 3]) # [R*8, 3] -> [R, 1, 8, 1, 3]\n",
    "        B = array_b.reshape([1, Rb, 1, NUM_ATOMS, 3]) # [R*8, 3] -> [1, R, 1, 8, 3]\n",
    "        \n",
    "        D = np.sqrt(np.sum((A-B)**2, axis=4, keepdims=True)) # [R, R, 8, 8, 1]\n",
    "        D = np.reshape(D, [Ra,Rb,NUM_ATOMS**2]) # [R, R, 8, 8, 1] -> [R, R, 64]\n",
    "        \n",
    "        \n",
    "    ### assume that input was \"mask\"\n",
    "    elif len(array_a_shape)==1:\n",
    "        Ra = len(array_a)//NUM_ATOMS\n",
    "        Rb = len(array_b)//NUM_ATOMS\n",
    "        A = array_a.reshape([Ra, 1, NUM_ATOMS, 1]) # [R*8] -> [R, 1, 8, 1]\n",
    "        B = array_b.reshape([1, Rb, 1, NUM_ATOMS]) # [R*8] -> [1, R, 1, 8] \n",
    "        D = A*B\n",
    "        D = np.reshape(D, [Ra,Rb,NUM_ATOMS**2])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_shape)\n",
    "    \n",
    "    \n",
    "    ### symmetrize\n",
    "    if Ra==Rb:\n",
    "        return symmetrize_LxLx64(D)\n",
    "    return D\n",
    "\n",
    "def LxLx64_tensor_to_L8xL8_matrix(tensor):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert flat (L x L x 64) matrix to big (L*8 x L*8) matrix\n",
    "    input:  tensor [L,L,64]\n",
    "    output: matrix [L*8, L*8]\n",
    "    \"\"\"\n",
    "\n",
    "    L8 = tensor.shape[0] * NUM_ATOMS\n",
    "\n",
    "    matrix = np.zeros([L8,L8])\n",
    "    idx = np.arange(0, L8, NUM_ATOMS)\n",
    "    ctr = 0\n",
    "    for i in range(0,NUM_ATOMS):\n",
    "        for j in range(0,NUM_ATOMS):\n",
    "            idx_i = idx + i\n",
    "            idx_j = idx + j\n",
    "            putmask1 = np.zeros([L8,L8])\n",
    "            putmask1[idx_i]=1\n",
    "            putmask2 = np.zeros([L8,L8])\n",
    "            putmask2[:,idx_j]=1\n",
    "            \n",
    "            matrix[np.logical_and(putmask1, putmask2)] = tensor[:,:,ctr].reshape(-1)\n",
    "            ctr += 1\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def matrix_to_coords(D):\n",
    "    \"\"\"\n",
    "    developed in 20201002_distance_matrix_to_coordinates_multidimensional_scaling.ipynb\n",
    "    \n",
    "    input:  D (distance matrix)\n",
    "    output: x (reconstructed coordinates)\n",
    "    \"\"\"\n",
    "    \n",
    "    D_sq = D**2\n",
    "    M = 0.5 * (D_sq[[0],:] + D_sq[:,[0]] - D_sq)\n",
    "    q,v = np.linalg.eigh(M) # numpy.linalg.eig often produces complex number with very small imaginary part.\n",
    "    x = np.sqrt(np.reshape(q[-3:],[1,3])) * v[:,-3:]\n",
    "    x = x[:,::-1] # numpy.linalg.eigh has largest eigenvalues last, so we need to flip the vector here...\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def L8xL8_matrix_to_Lx3_coords(matrix, mask):\n",
    "\n",
    "    matrix_reduced = matrix[mask==1,:][:,mask==1]\n",
    "    coords_reduced = matrix_to_coords(matrix_reduced)\n",
    "    \n",
    "    L8 = matrix.shape[0]\n",
    "    coords = np.zeros([L8,3])    \n",
    "    coords[mask==1] = coords_reduced\n",
    "    return coords\n",
    "\n",
    "\n",
    "def superimposer(x,y, mask=None):\n",
    "    \n",
    "    if mask is not None:\n",
    "        x = x[mask==1]\n",
    "        y = y[mask==1]\n",
    "        \n",
    "    xn = x - np.mean(x,axis=0)\n",
    "    yn = y - np.mean(y,axis=0)    \n",
    "    \n",
    "    C = np.matmul(xn.T,yn)\n",
    "\n",
    "    U, s, V = np.linalg.svd(C, full_matrices=1, compute_uv=1)\n",
    "    R = np.matmul(U,V)\n",
    "    \n",
    "    yn_proj = np.matmul(yn, R.T)\n",
    "    \n",
    "    diff = xn - yn_proj\n",
    "    rms = np.sqrt(sum(sum(diff * diff)) / x.shape[0])\n",
    "    \n",
    "    \n",
    "    return rms, xn, yn_proj\n",
    "\n",
    "\n",
    "def get_mask_for_nucleotide(nuc):\n",
    "    \n",
    "    \"\"\"\n",
    "    input: nucleotide (string; either 'A', 'C', 'G', 'U')\n",
    "    output: mask (numpy array of shape [8])\n",
    "    \n",
    "    Atoms are in the following order:\n",
    "    0   P\n",
    "    1   C4'a\n",
    "    2   C2-GA_Pur\n",
    "    3   C6\n",
    "    4   N9\n",
    "    5   C2-CU_Pyr\n",
    "    6   C4\n",
    "    7   N1\n",
    "    \"\"\"\n",
    "    \n",
    "    if nuc not in list('ACGU'):\n",
    "        raise Exception('Nucleotide must be either A, C, G or U but is:', nuc)\n",
    "        \n",
    "    mask = np.zeros([8])\n",
    "    mask[:2] = 1\n",
    "    if nuc in list('GA'):\n",
    "        mask[2:5]=1\n",
    "        return mask\n",
    "    elif nuc in list('CU'):\n",
    "        mask[5:]=1\n",
    "        return mask\n",
    "    \n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "def get_mask_for_sequence(seq):\n",
    "    return np.concatenate([get_mask_for_nucleotide(nuc) for nuc in seq], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_3D_tensor(array_a, *args):\n",
    "    \n",
    "    \"\"\"\n",
    "    originally developed in:\n",
    "    /export/chrisblum/projects/main/rna_3d/data_loaders_and_conversion/20200713_develop_data_augmentation_data_loader.ipynb\n",
    "    and modified in:\n",
    "    /alto/chris/projects/main/rna_3d/reconstruct_3d_from_distances/20200728_LL64_to_L3_Julius_reconstructions_faster_mirrorfix_symmetrizationfix.ipynb\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    input(s):\n",
    "        either \n",
    "            coords [Ra*8, 3] and optionally [Rb*8, 3]\n",
    "        or\n",
    "            mask [Ra*8] and optionally [Rb*8]\n",
    "        \n",
    "    output:\n",
    "        either\n",
    "            D [Ra, Ra, 64]\n",
    "        or\n",
    "            D [Ra, Rb, 64]\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if len(args)==1:\n",
    "        array_b = args[0]\n",
    "    else:\n",
    "        array_b = array_a\n",
    "    \n",
    "    array_a_shape = array_a.shape\n",
    "    array_b_shape = array_b.shape\n",
    "    if array_a_shape[0]%NUM_ATOMS!=0:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_a_shape)\n",
    "    if array_b_shape[0]%NUM_ATOMS!=0:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_b_shape)        \n",
    "    \n",
    "    if len(array_a_shape)!=len(array_b_shape):\n",
    "        raise Exception('Inputs have different dimensions:', array_a_shape, array_b_shape)  \n",
    "    \n",
    "    ### assume that input was \"coords\"\n",
    "    if len(array_a_shape)==2:\n",
    "        Ra = len(array_a)//NUM_ATOMS\n",
    "        Rb = len(array_b)//NUM_ATOMS\n",
    "        A = array_a.reshape([Ra, 1, NUM_ATOMS, 1, 3]) # [R*8, 3] -> [R, 1, 8, 1, 3]\n",
    "        B = array_b.reshape([1, Rb, 1, NUM_ATOMS, 3]) # [R*8, 3] -> [1, R, 1, 8, 3]\n",
    "        \n",
    "        D = np.sqrt(np.sum((A-B)**2, axis=4, keepdims=True)) # [R, R, 8, 8, 1]\n",
    "        D = np.reshape(D, [Ra,Rb,NUM_ATOMS**2]) # [R, R, 8, 8, 1] -> [R, R, 64]\n",
    "        \n",
    "        \n",
    "    ### assume that input was \"mask\"\n",
    "    elif len(array_a_shape)==1:\n",
    "        Ra = len(array_a)//NUM_ATOMS\n",
    "        Rb = len(array_b)//NUM_ATOMS\n",
    "        A = array_a.reshape([Ra, 1, NUM_ATOMS, 1]) # [R*8] -> [R, 1, 8, 1]\n",
    "        B = array_b.reshape([1, Rb, 1, NUM_ATOMS]) # [R*8] -> [1, R, 1, 8] \n",
    "        D = A*B\n",
    "        D = np.reshape(D, [Ra,Rb,NUM_ATOMS**2])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_shape)\n",
    "    \n",
    "    \n",
    "    ### symmetrize\n",
    "    if Ra==Rb:\n",
    "        return symmetrize(D)\n",
    "    return D\n",
    "\n",
    "def symmetrize(D):\n",
    "    \"\"\"\n",
    "    developed in /alto/chris/projects/main/rna_3d/reconstruct_3d_from_distances/20200728_reverse_symmetrization.ipynb\n",
    "    input:  D [L,L,64]\n",
    "    output: D [L,L,64]\n",
    "    \n",
    "    If this function is applied to an unsymmetrized tensor, it will be symmetrized.\n",
    "    If this function is applied to a symmetrized tensor, it will be un-symmetrized!\n",
    "    \n",
    "    \"\"\"    \n",
    "    upper = np.transpose(np.triu(np.transpose(D,[2,0,1]), 0),[1,2,0])  # 0 includes diagonal\n",
    "    lower = np.transpose(np.tril(np.transpose(D,[2,0,1]), -1),[1,2,0]) # 0 includes diagonal\n",
    "    return upper + lower[:,:,symmetrization_indices]\n",
    "\n",
    "\n",
    "def get_3D_tensor(array_a, *args):\n",
    "    \n",
    "    \"\"\"\n",
    "    input(s):\n",
    "        either \n",
    "            coords [Ra*8, 3] and optionally [Rb*8, 3]\n",
    "        or\n",
    "            mask [Ra*8] and optionally [Rb*8]\n",
    "        \n",
    "    output:\n",
    "        either\n",
    "            D [Ra, Ra, 64]\n",
    "        or\n",
    "            D [Ra, Rb, 64]\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if len(args)==1:\n",
    "        array_b = args[0]\n",
    "    else:\n",
    "        array_b = array_a\n",
    "    \n",
    "    array_a_shape = array_a.shape\n",
    "    array_b_shape = array_b.shape\n",
    "    if array_a_shape[0]%NUM_ATOMS!=0:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_a_shape)\n",
    "    if array_b_shape[0]%NUM_ATOMS!=0:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_b_shape)        \n",
    "    \n",
    "    if len(array_a_shape)!=len(array_b_shape):\n",
    "        raise Exception('Inputs have different dimensions:', array_a_shape, array_b_shape)  \n",
    "    \n",
    "    ### assume that input was \"coords\"\n",
    "    if len(array_a_shape)==2:\n",
    "        Ra = len(array_a)//NUM_ATOMS\n",
    "        Rb = len(array_b)//NUM_ATOMS\n",
    "        A = array_a.reshape([Ra, 1, NUM_ATOMS, 1, 3]) # [R*8, 3] -> [R, 1, 8, 1, 3]\n",
    "        B = array_b.reshape([1, Rb, 1, NUM_ATOMS, 3]) # [R*8, 3] -> [1, R, 1, 8, 3]\n",
    "        \n",
    "        D = np.sqrt(np.sum((A-B)**2, axis=4, keepdims=True)) # [R, R, 8, 8, 1]\n",
    "        D = np.reshape(D, [Ra,Rb,NUM_ATOMS**2]) # [R, R, 8, 8, 1] -> [R, R, 64]\n",
    "        \n",
    "        \n",
    "    ### assume that input was \"mask\"\n",
    "    elif len(array_a_shape)==1:\n",
    "        Ra = len(array_a)//NUM_ATOMS\n",
    "        Rb = len(array_b)//NUM_ATOMS\n",
    "        A = array_a.reshape([Ra, 1, NUM_ATOMS, 1]) # [R*8] -> [R, 1, 8, 1]\n",
    "        B = array_b.reshape([1, Rb, 1, NUM_ATOMS]) # [R*8] -> [1, R, 1, 8] \n",
    "        D = A*B\n",
    "        D = np.reshape(D, [Ra,Rb,NUM_ATOMS**2])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_shape)\n",
    "    \n",
    "    \n",
    "    ### symmetrize\n",
    "    if Ra==Rb:\n",
    "        triu           = np.reshape(np.triu(np.ones([Ra,Ra]), 1), [Ra,Ra,1])\n",
    "        triu_plus_diag = np.reshape(np.triu(np.ones([Ra,Ra]), 0), [Ra,Ra,1])\n",
    "        D              = D*triu_plus_diag + np.transpose(D*triu, axes=[1,0,2])        \n",
    "\n",
    "    return D\n",
    "\n",
    "def get_substructure_position(coords, substructure_length):\n",
    "    \n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        (R is the number of residues)\n",
    "        coords:              [R*8, 3]\n",
    "        substructure_length: requested number of residues in substructure, must be smaller than R\n",
    "    \n",
    "    output:\n",
    "        tuple consisting of:\n",
    "            i: integer indicating the starting position of the substructure in the structure\n",
    "            substructure_length: just the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    R = len(coords)//NUM_ATOMS\n",
    "    if R<=substructure_length:    \n",
    "        return (None, substructure_length)\n",
    "    else:\n",
    "        i = np.random.randint(0,R-substructure_length+1) # random index\n",
    "        return (i, substructure_length)\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_substructure(coords, mask, i, return_contact_mask=True, cutoff=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Developed 2020-03-30 in:\n",
    "    /projects/main/rna_3d/data_loaders_and_conversion/20200330_develop_data_augmentation_data_loader.ipynb\n",
    "    \n",
    "    inputs:\n",
    "        (R is the number of residues)\n",
    "        coords: [R*8, 3]\n",
    "        mask:   [R*8]\n",
    "        i:      tuple of (starting position, substructure_length)\n",
    "        \n",
    "    outputs:\n",
    "        (Rs is the number of residues in the substructure)\n",
    "        coords: [Rs*8, 3]\n",
    "        mask:   [Rs*8]\n",
    "        contact_mask: [Rs] (only if return_contact_mask=True)\n",
    "    \"\"\"    \n",
    "    \n",
    "    (i, Rs) = i\n",
    "    \n",
    "    if len(np.shape(coords))!=2 or len(np.shape(mask))!=1:\n",
    "        raise Exception('Input arrays shoudl have shapes [R*8, 3] and [R*8] but have:', np.shape(coords), np.shape(mask))\n",
    "    \n",
    "    lengths = [len(x) for x in [coords, mask]]\n",
    "    u = np.unique(lengths)\n",
    "    if len(u)>1:\n",
    "        raise Exception('Input arrays have different size along dimension 0:', lengths)    \n",
    "    if u%NUM_ATOMS!=0:\n",
    "        raise Exception('Input dimension must be divisible by %d but is %d' %(NUM_ATOMS, u))\n",
    "        \n",
    "    if i==None:\n",
    "        return coords, mask\n",
    "    \n",
    "    \n",
    "\n",
    "    else:\n",
    "        ### randomly select substructure\n",
    "        sel_substr = np.arange(i*NUM_ATOMS, (i+Rs)*NUM_ATOMS)\n",
    "        \n",
    "        if len(sel_substr)%NUM_ATOMS!=0:\n",
    "            raise Exception(len(sel_substr))\n",
    "\n",
    "        ### selector for matrix (remaining residues)\n",
    "        sel_matrix = np.arange(len(coords))\n",
    "        sel_matrix = np.delete(sel_matrix, sel_substr)\n",
    "\n",
    "        ### select coords and mask\n",
    "        coords_substr = coords[sel_substr]\n",
    "        coords_matrix = coords[sel_matrix]\n",
    "        mask_substr   = mask[sel_substr]\n",
    "        mask_matrix   = mask[sel_matrix]\n",
    "\n",
    "        outputs = [coords_substr, mask_substr]\n",
    "        \n",
    "        if return_contact_mask:\n",
    "            D = get_3D_tensor(coords_substr, coords_matrix) # [R1, R2, 64]\n",
    "            M = get_3D_tensor(mask_substr,   mask_matrix)   # [R1, R2, 64]\n",
    "\n",
    "            # Crucial!! This sets masked values to really large numbers!\n",
    "            # This is because we take the minimum later, but \"min\" does not know which values to ignore.\n",
    "            D[M==0] = 10**6 \n",
    "            D_min   = np.min(D, axis=2) # [R1, R2, 64] -> [R1, R2], use a single atom as a proxy for the whole residue\n",
    "\n",
    "            contact_mask = np.any(D_min<cutoff, axis=1)*1 # [R1]\n",
    "            outputs.append(contact_mask)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "def enc2str(valid_sq):\n",
    "    vseq = ''.join(enc_seq_pdb.decode(valid_sq[0]))\n",
    "    s = ''\n",
    "    for ss in vseq:\n",
    "        if ss=='<':\n",
    "            break\n",
    "        else:\n",
    "            s+=ss\n",
    "    return s\n",
    "\n",
    "\n",
    "def transposeD(tJ):\n",
    "    tjlist = []\n",
    "    for m in range(64):\n",
    "        tmp = (tJ[:,:,m]    + tJ[:,:,m].T) * 0.5\n",
    "        tjlist.append(tmp)\n",
    "    return np.stack(tjlist,-1)\n",
    "\n",
    "\n",
    "def np_pad(matrix, pad_max=100, pad_val=0, max_depth=64):\n",
    "    M_pad = np.full([pad_max, pad_max, max_depth], pad_val, dtype=float)\n",
    "    M_pad[:matrix.shape[0], :matrix.shape[1], :] = matrix\n",
    "    return M_pad\n",
    "\n",
    "\n",
    "atom_list = [\"P\",\"C4'\",\"C2-GA_Pur\",\"C6\",\"N9\",\"C2-CU_Pyr\",\"C4\",\"N1\"]\n",
    "atid = dict(zip(atom_list, np.arange(len(atom_list)))) # atom to index dict\n",
    "NUM_ATOMS = len(atom_list)\n",
    "print(NUM_ATOMS)\n",
    "\n",
    "\n",
    "map_idx_to_atom = dict(zip(np.arange(NUM_ATOMS), [\"P\",\"C4'\",\"C2\",\"C6\",\"N9\",\"C2\",\"C4\",\"N1\"])) # atom to index dict\n",
    "atom_types = list(map_idx_to_atom.values())\n",
    "atom_type_dict = dict(zip(atom_types, [x[0] for x in atom_types]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_check_dict(check_dict):\n",
    "    \"\"\"\n",
    "    Version: 2020-02-17\n",
    "    Developed in: /projects/main/rna_3d/simrna_drift_dataset/20200212_SimRNA_drift_ramdisk_RMSD_CHECKS.ipynb\n",
    "    \"\"\"    \n",
    "    \n",
    "    allowed = [set([\"P\",\"C4'\",\"C2\",\"C6\",\"N9\"]),\n",
    "               set([\"P\",\"C4'\",\"C2\",\"C4\",\"N1\"])]\n",
    "    \n",
    "    for key, atoms in check_dict.items():\n",
    "        if not any([set(atoms) == x for x in allowed]):\n",
    "            print('Residue does not have all necessary atoms', key, atoms)\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_8_atom_structure_to_pdb_file(filename, coords, mask, seq, occupancy, temperature_factor):\n",
    "    \"\"\"\n",
    "    Version: 2020-02-17\n",
    "    Developed in: /projects/main/rna_3d/simrna_drift_dataset/20200212_SimRNA_drift_ramdisk_RMSD_CHECKS.ipynb\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(coords)//NUM_ATOMS>max_len:\n",
    "        print('Did not write (structure too > max_len residues)', filename)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    lines_list = []\n",
    "    #lines_list.append('MODEL        1                                                                  \\n')\n",
    "\n",
    "    seq_list = list(seq)\n",
    "    atom_number = 0\n",
    "    check_dict = dict()\n",
    "    \n",
    "    for k, (c, m, occ, temp) in enumerate(zip(coords, mask, occupancy, temperature_factor)):\n",
    "\n",
    "        if temp==0:\n",
    "            temp=10\n",
    "        \n",
    "        within_residue_idx  = k % NUM_ATOMS  # 0,1,2,3,4,5,6, 0,1,2,3,4,5,6, 0,1,2,3,4,5,6, ...\n",
    "        within_sequence_idx = k // NUM_ATOMS # 0,0,0,0,0,0,0, 1,1,1,1,1,1,1, 2,2,2,2,2,2,2, ... \n",
    "\n",
    "\n",
    "        ### if no data for first phosphate: append \"fake\" phosphate\n",
    "        if k==0 and not m:\n",
    "            atom_number += 1\n",
    "            residue_name = seq_list[within_sequence_idx]\n",
    "            atom_name    = 'P'\n",
    "            atom_type    = 'P'\n",
    "\n",
    "            res1_C4prime = coords[1]\n",
    "            res2_P       = coords[8]\n",
    "            c            = res1_C4prime + (res1_C4prime-res2_P)\n",
    "\n",
    "            string_input = (atom_number, atom_name, residue_name, within_sequence_idx+1, \n",
    "                            c[0], c[1], c[2], 1.0, 10.0, '', atom_type)\n",
    "            line = 'ATOM  %5d%5s%4s A%4d    %8.3f%8.3f%8.3f%6.2f%6.2f          %2s%2s\\n' % string_input\n",
    "            lines_list.append(line)  \n",
    "            \n",
    "            if within_sequence_idx not in check_dict:\n",
    "                check_dict[within_sequence_idx] = []\n",
    "            check_dict[within_sequence_idx].append(atom_name)                \n",
    "\n",
    "        if m:\n",
    "            atom_number += 1\n",
    "            residue_name = seq_list[within_sequence_idx]\n",
    "            atom_name    = map_idx_to_atom[within_residue_idx]\n",
    "            atom_type    = atom_type_dict[atom_name]\n",
    "\n",
    "            string_input = (atom_number, atom_name, residue_name, within_sequence_idx+1, \n",
    "                            c[0], c[1], c[2], occ, temp, '', atom_type)\n",
    "            line = 'ATOM  %5d%5s%4s A%4d    %8.3f%8.3f%8.3f%6.2f%6.2f          %2s%2s\\n' % string_input\n",
    "            lines_list.append(line)\n",
    "\n",
    "            ### also create last line in case this really is the last line\n",
    "            string_input = (atom_number+1, '', residue_name, within_sequence_idx+1)                \n",
    "            last_line = 'TER   %5d%5s%4s A%4d                                                      \\n' % string_input\n",
    "            \n",
    "            if within_sequence_idx not in check_dict:\n",
    "                check_dict[within_sequence_idx] = []\n",
    "            check_dict[within_sequence_idx].append(atom_name)                \n",
    "\n",
    "    lines_list.append(last_line)\n",
    "    #lines_list.append('ENDMDL                                                                          \\n')\n",
    "    lines_list.append('END                                                                             \\n')\n",
    "    \n",
    "    \n",
    "    if check_check_dict(check_dict):\n",
    "        if filename is not None:\n",
    "            with open(filename, 'w') as fileobj:\n",
    "                for line in lines_list:\n",
    "                    fileobj.write(line)\n",
    "        return lines_list\n",
    "\n",
    "    else:\n",
    "        print('Did not write', filename)\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "traindata_df['cluster_size'] = traindata_df['len']\n",
    "valdata_df['cluster_size']   = valdata_df['len']\n",
    "testdata_df['cluster_size']  = testdata_df['len']\n",
    "\n",
    "traindata_df['num_models'] = traindata_df['len']\n",
    "valdata_df['num_models']   = valdata_df['len']\n",
    "testdata_df['num_models']  = testdata_df['len']\n",
    "\n",
    "\n",
    "def conv2d_layer(pad_in, n_filers=None, conv_name=None, k=None, args=None, raw_sq_input=None):\n",
    "    \n",
    "    #if k is None:\n",
    "    #    k = params[\"resnet_k\"]\n",
    "\n",
    "    pad_val = (k-1)//2\n",
    "    paddings = tf.constant([[0,0], [pad_val, pad_val], [pad_val, pad_val], [0,0]])\n",
    "    padded_inputs = tf.pad(tensor=pad_in,\n",
    "                           paddings=paddings,\n",
    "                           mode=\"CONSTANT\",\n",
    "                           constant_values=0)\n",
    "    \n",
    "    conv = tf.layers.conv2d(inputs=padded_inputs,\n",
    "                            filters=n_filers,\n",
    "                            kernel_size=[k,k],\n",
    "                            strides=[1,1],\n",
    "                            padding=\"VALID\",\n",
    "                            use_bias=False, \n",
    "                            kernel_initializer=tf.initializers.variance_scaling(),\n",
    "                            name=conv_name)\n",
    "    return conv\n",
    "\n",
    "def residual_stack(h, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "  h += tf.transpose(h[:,:,:,:],[0,1,2,3])\n",
    "  h *= 0.5\n",
    "  for i in range(num_residual_layers):\n",
    "    h_i = tf.nn.relu(h)\n",
    "\n",
    "    h_i = conv2d_layer(h_i, num_residual_hiddens, \"res3x3_%d\" % i, 7)\n",
    "    \n",
    "    #snt.Conv2D(\n",
    "    #    output_channels=num_residual_hiddens,\n",
    "    #    kernel_shape=(7, 7),\n",
    "    #    stride=(1, 1),\n",
    "    #    name=\"res3x3_%d\" % i)(h_i)\n",
    "    h_i = tf.nn.relu(h_i)\n",
    "    h_i = conv2d_layer(h_i, num_residual_hiddens, \"res1x1_%d\" % i, 1)\n",
    "    #h_i = snt.Conv2D(\n",
    "    #    output_channels=num_hiddens,\n",
    "    #    kernel_shape=(1, 1),\n",
    "    #    stride=(1, 1),\n",
    "    #    name=\"res1x1_%d\" % i)(h_i)\n",
    "    h += h_i\n",
    "  return tf.nn.relu(h)\n",
    "\n",
    "class Encoder(snt.AbstractModule):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name='encoder'):\n",
    "    super(Encoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "    \n",
    "  def _build(self, x):\n",
    "    h = conv2d_layer(x, self._num_hiddens / 2, \"enc_1\", 7)\n",
    "    #h = snt.Conv2D(\n",
    "    #    output_channels=self._num_hiddens / 2,\n",
    "    #    kernel_shape=(7, 7),\n",
    "    #    stride=(1, 1),\n",
    "    #    name=\"enc_1\")(x)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = conv2d_layer(x, self._num_hiddens, \"enc_2\", 4)\n",
    "    #h = snt.Conv2D(\n",
    "    #    output_channels=self._num_hiddens,\n",
    "    #    kernel_shape=(4, 4),\n",
    "    #    stride=(1, 1),\n",
    "    #    name=\"enc_2\")(h)\n",
    "    h = tf.nn.relu(h) \n",
    "    h = conv2d_layer(x, self._num_hiddens, \"enc_3\", 3)\n",
    "    #h = snt.Conv2D(\n",
    "    #    output_channels=self._num_hiddens,\n",
    "    #    kernel_shape=(3, 3),\n",
    "    #    stride=(1, 1),\n",
    "    #    name=\"enc_3\")(h)\n",
    "    \n",
    "    h = residual_stack(\n",
    "        h,\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "    return (h)\n",
    "\n",
    "class Decoder(snt.AbstractModule):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name='decoder'):\n",
    "    super(Decoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "  \n",
    "  def _build(self, x):\n",
    "    h = conv2d_layer(x, self._num_hiddens, \"dec_1\", 7)\n",
    "    #h = snt.Conv2D(\n",
    "    #  output_channels=self._num_hiddens,\n",
    "    #  kernel_shape=(7, 7),\n",
    "    #  stride=(1, 1),\n",
    "    #  name=\"dec_1\")(x)\n",
    "\n",
    "    h = residual_stack(\n",
    "        h,\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "\n",
    "    h = snt.Conv2DTranspose(\n",
    "        output_channels=int(self._num_hiddens / 2),\n",
    "        output_shape=None,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(1, 1),\n",
    "        name=\"dec_2\")(h)\n",
    "    h = tf.nn.relu(h)\n",
    "\n",
    "    x_recon = snt.Conv2DTranspose(\n",
    "        output_channels=PARAMS['max_depth'],\n",
    "        output_shape=None,\n",
    "        kernel_shape=(2, 2),\n",
    "        stride=(1, 1),\n",
    "        name=\"dec_3\")(h)\n",
    "    #x_recon = conv2d_layer(x_recon, 49, \"dec_4\", 2)\n",
    "\n",
    "    return x_recon\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# commitment_cost should be set appropriately. It's often useful to try a couple\n",
    "# of values. It mostly depends on the scale of the reconstruction cost\n",
    "# (log p(x|z)). So if the reconstruction cost is 100x higher, the\n",
    "# commitment_cost should also be multiplied with the same amount.\n",
    "commitment_cost = 0.25\n",
    "# Use EMA updates for the codebook (instead of the Adam optimizer).\n",
    "# This typically converges faster, and makes the model less dependent on choice\n",
    "# of the optimizer. In the VQ-VAE paper EMA updates were not used (but was\n",
    "# developed afterwards). See Appendix of the paper for more details.\n",
    "vq_use_ema = True\n",
    "# This is only used for EMA updates.\n",
    "decay = 0.99\n",
    "learning_rate = 3e-5\n",
    "_BATCH_NORM_DECAY = 0.99\n",
    "_BATCH_NORM_EPSILON = 1e-3\n",
    "\n",
    "def batch_norm(inputs, training, data_format, batch_norm_name):\n",
    "    return tf.layers.batch_normalization(inputs=inputs, momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n",
    "                                         scale=True, training=training, fused=True, name=batch_norm_name)\n",
    "\n",
    "def block_layer(pos, inputs, num_hiddens, training):\n",
    "\n",
    "    shortcut = inputs\n",
    "    inputs = batch_norm(inputs, training, 'channels_last', 'batch_norm_' + str(pos) + '_0')\n",
    "    inputs = tf.nn.relu(inputs)\n",
    "    inputs = conv2d_layer(inputs, num_hiddens, 'conv_' + str(pos) + '_0', 5)\n",
    "    inputs = batch_norm(inputs, training, 'channels_last', 'batch_norm_' + str(pos) + '_1')\n",
    "    inputs = tf.nn.relu(inputs)\n",
    "    inputs = conv2d_layer(inputs, num_hiddens, 'conv_' + str(pos) + '_1', 3)\n",
    "    return inputs + shortcut\n",
    "\n",
    "def dec_block_layer(pos, inputs, num_hiddens):\n",
    "\n",
    "    shortcut = inputs\n",
    "    #inputs = batch_norm(inputs, training, 'channels_last', 'batch_norm_' + str(pos) + '_0')\n",
    "    inputs = tf.nn.relu(inputs)\n",
    "    inputs = conv2d_layer(inputs, num_hiddens, 'conv_' + str(pos) + '_0', 5)\n",
    "    #inputs = batch_norm(inputs, training, 'channels_last', 'batch_norm_' + str(pos) + '_1')\n",
    "    inputs = tf.nn.relu(inputs)\n",
    "    inputs = conv2d_layer(inputs, num_hiddens, 'conv_' + str(pos) + '_1', 3)\n",
    "    #resout = inputs + shortcut\n",
    "    return inputs + shortcut\n",
    "\n",
    "class Encoder(snt.AbstractModule):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name='encoder'):\n",
    "    super(Encoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "    \n",
    "  def _build(self, x, train_ph):\n",
    "    \n",
    "    h = conv2d_layer(x, self._num_hiddens*2, \"enc_1\", 7)\n",
    "    print(1,h.shape)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = conv2d_layer(h, self._num_hiddens, \"enc_2\", 5)\n",
    "    print(2,h.shape)\n",
    "\n",
    "    h = tf.nn.relu(h) \n",
    "    h = conv2d_layer(h, self._num_residual_hiddens, \"enc_3\", 3)\n",
    "    print(3,h.shape)\n",
    "\n",
    "    h = block_layer(0, h, self._num_residual_hiddens, train_ph)\n",
    "    print(4,h.shape)\n",
    "    #h = block_layer(1, h, self._num_residual_hiddens, train_ph)\n",
    "    print(5,h.shape)\n",
    "    #h = block_layer(2, h, self._num_residual_hiddens, train_ph)\n",
    "    #print(6,h.shape)\n",
    "    print('----------')\n",
    "    return h\n",
    "\n",
    "class Decoder(snt.AbstractModule):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name='decoder'):\n",
    "    super(Decoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "  \n",
    "  def _build(self, x, features_mask_placeholder):\n",
    "    d = conv2d_layer(x, self._num_hiddens / 2, \"dec_1\", 7)\n",
    "    print(1,d.shape)\n",
    "    d = dec_block_layer(3, d, self._num_hiddens / 2)\n",
    "    print(2,d.shape)\n",
    "    d = conv2d_layer(d, self._num_hiddens , \"dec_2\", 5)\n",
    "    print(3,d.shape)\n",
    "    #d = tf.nn.relu(d) \n",
    "    d = dec_block_layer(4, d, self._num_hiddens )\n",
    "    print(4,d.shape)\n",
    "    d = conv2d_layer(d, self._num_hiddens*2 , \"dec_3\", 5)\n",
    "    print(5,d.shape)\n",
    "    d = conv2d_layer(d, self._num_hiddens*4 , \"dec_3b\", 5)\n",
    "    print(5,d.shape)\n",
    "    d = dec_block_layer(5, d, self._num_hiddens*4)\n",
    "    print(5,d.shape)\n",
    "    #d = conv2d_layer(d, self._num_residual_hiddens , \"dec_4\", 3) * features_mask_placeholder\n",
    "    print(6,d.shape)\n",
    "    x_recon = dec_block_layer(6, d, self._num_hiddens * 4 ) * features_mask_placeholder\n",
    "    return tf.nn.relu(x_recon) \n",
    "\n",
    "def get_lr(learning_rate, step, lr_decay):\n",
    "    learning_rate *= lr_decay ** step\n",
    "    return learning_rate\n",
    "\n",
    "def get_session(sess):\n",
    "    session = sess\n",
    "    while type(session).__name__ != 'Session':\n",
    "        #pylint: disable=W0212\n",
    "        session = session._sess\n",
    "    return session\n",
    "\n",
    "def np_pad(matrix, pad_max=PARAMS['RNA_len'], pad_val=0, max_depth=64):\n",
    "    M_pad = np.full([pad_max, pad_max, max_depth], pad_val, dtype=float)\n",
    "    M_pad[:matrix.shape[0], :matrix.shape[1], :] = matrix\n",
    "    return M_pad\n",
    "\n",
    "def np_pad_mask(matrix, pad_max=PARAMS['RNA_len'], pad_val=0, max_depth=64):\n",
    "    M_pad = np.full([pad_max, pad_max, max_depth], pad_val, dtype=float)\n",
    "    M_pad[:matrix.shape[0], :matrix.shape[1], :] = matrix\n",
    "    return M_pad\n",
    "\n",
    "def batch_data(batch_size):\n",
    "    idx = [np.random.choice(len(data)) for k in range(batch_size)]\n",
    "    return np.stack([np_pad(features[_]) for _ in idx]), np.stack([np_pad_mask(features_mask[_]) for _ in idx])\n",
    "    #return np.stack([np_pad(data['big_matrix_flat'][_]) for _ in idx]), np.stack([np_pad(data['big_mask_flat'][_]) for _ in idx])\n",
    "\n",
    "def batch_data_seq(batch_size):\n",
    "    idx = [np.random.choice(len(data)) for k in range(batch_size)]\n",
    "    return np.stack([seqs[_] for _ in idx]), np.stack([np_pad(features[_]) for _ in idx]), np.stack([np_pad_mask(features_mask[_]) for _ in idx])\n",
    "\n",
    "def batch_data_seq_nrnd(batch_size, idx2):\n",
    "    idx = [idx2 for k in range(batch_size)]\n",
    "    return np.stack([seqs[_] for _ in idx]), np.stack([np_pad(features[_]) for _ in idx]), np.stack([np_pad_mask(features_mask[_]) for _ in idx])\n",
    "\n",
    "def get_3D_tensor(array_a, *args):\n",
    "    \n",
    "    \"\"\"\n",
    "    input(s):\n",
    "        either \n",
    "            coords [Ra*8, 3] and optionally [Rb*8, 3]\n",
    "        or\n",
    "            mask [Ra*8] and optionally [Rb*8]\n",
    "        \n",
    "    output:\n",
    "        either\n",
    "            D [Ra, Ra, 64]\n",
    "        or\n",
    "            D [Ra, Rb, 64]\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if len(args)==1:\n",
    "        array_b = args[0]\n",
    "    else:\n",
    "        array_b = array_a\n",
    "    \n",
    "    array_a_shape = array_a.shape\n",
    "    array_b_shape = array_b.shape\n",
    "    if array_a_shape[0]%NUM_ATOMS!=0:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_a_shape)\n",
    "    if array_b_shape[0]%NUM_ATOMS!=0:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_b_shape)        \n",
    "    \n",
    "    if len(array_a_shape)!=len(array_b_shape):\n",
    "        raise Exception('Inputs have different dimensions:', array_a_shape, array_b_shape)  \n",
    "    \n",
    "    ### assume that input was \"coords\"\n",
    "    if len(array_a_shape)==2:\n",
    "        Ra = len(array_a)//NUM_ATOMS\n",
    "        Rb = len(array_b)//NUM_ATOMS\n",
    "        A = array_a.reshape([Ra, 1, NUM_ATOMS, 1, 3]) # [R*8, 3] -> [R, 1, 8, 1, 3]\n",
    "        B = array_b.reshape([1, Rb, 1, NUM_ATOMS, 3]) # [R*8, 3] -> [1, R, 1, 8, 3]\n",
    "        \n",
    "        D = np.sqrt(np.sum((A-B)**2, axis=4, keepdims=True)) # [R, R, 8, 8, 1]\n",
    "        D = np.reshape(D, [Ra,Rb,NUM_ATOMS**2]) # [R, R, 8, 8, 1] -> [R, R, 64]\n",
    "        \n",
    "        \n",
    "    ### assume that input was \"mask\"\n",
    "    elif len(array_a_shape)==1:\n",
    "        Ra = len(array_a)//NUM_ATOMS\n",
    "        Rb = len(array_b)//NUM_ATOMS\n",
    "        A = array_a.reshape([Ra, 1, NUM_ATOMS, 1]) # [R*8] -> [R, 1, 8, 1]\n",
    "        B = array_b.reshape([1, Rb, 1, NUM_ATOMS]) # [R*8] -> [1, R, 1, 8] \n",
    "        D = A*B\n",
    "        D = np.reshape(D, [Ra,Rb,NUM_ATOMS**2])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        raise Exception('Shape must be [R*8, 3] or [R*8], but is', array_shape)\n",
    "    \n",
    "    \n",
    "    ### symmetrize\n",
    "    if Ra==Rb:\n",
    "        triu           = np.reshape(np.triu(np.ones([Ra,Ra]), 1), [Ra,Ra,1])\n",
    "        triu_plus_diag = np.reshape(np.triu(np.ones([Ra,Ra]), 0), [Ra,Ra,1])\n",
    "        D              = D*triu_plus_diag + np.transpose(D*triu, axes=[1,0,2])        \n",
    "\n",
    "    return D\n",
    "\n",
    "def get_substructure_position(coords, substructure_length):\n",
    "    \n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        (R is the number of residues)\n",
    "        coords:              [R*8, 3]\n",
    "        substructure_length: requested number of residues in substructure, must be smaller than R\n",
    "    \n",
    "    output:\n",
    "        tuple consisting of:\n",
    "            i: integer indicating the starting position of the substructure in the structure\n",
    "            substructure_length: just the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    R = len(coords)//NUM_ATOMS\n",
    "    if R<=substructure_length:    \n",
    "        return (None, substructure_length)\n",
    "    else:\n",
    "        i = np.random.randint(0,R-substructure_length+1) # random index\n",
    "        return (i, substructure_length)\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_substructure(coords, mask, i, return_contact_mask=True, cutoff=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Developed 2020-03-30 in:\n",
    "    /projects/main/rna_3d/data_loaders_and_conversion/20200330_develop_data_augmentation_data_loader.ipynb\n",
    "    \n",
    "    inputs:\n",
    "        (R is the number of residues)\n",
    "        coords: [R*8, 3]\n",
    "        mask:   [R*8]\n",
    "        i:      tuple of (starting position, substructure_length)\n",
    "        \n",
    "    outputs:\n",
    "        (Rs is the number of residues in the substructure)\n",
    "        coords: [Rs*8, 3]\n",
    "        mask:   [Rs*8]\n",
    "        contact_mask: [Rs] (only if return_contact_mask=True)\n",
    "    \"\"\"    \n",
    "    \n",
    "    (i, Rs) = i\n",
    "    \n",
    "    if len(np.shape(coords))!=2 or len(np.shape(mask))!=1:\n",
    "        raise Exception('Input arrays shoudl have shapes [R*8, 3] and [R*8] but have:', np.shape(coords), np.shape(mask))\n",
    "    \n",
    "    lengths = [len(x) for x in [coords, mask]]\n",
    "    u = np.unique(lengths)\n",
    "    if len(u)>1:\n",
    "        raise Exception('Input arrays have different size along dimension 0:', lengths)    \n",
    "    if u%NUM_ATOMS!=0:\n",
    "        raise Exception('Input dimension must be divisible by %d but is %d' %(NUM_ATOMS, u))\n",
    "        \n",
    "    if i==None:\n",
    "        return coords, mask\n",
    "    \n",
    "    \n",
    "\n",
    "    else:\n",
    "        ### randomly select substructure\n",
    "        sel_substr = np.arange(i*NUM_ATOMS, (i+Rs)*NUM_ATOMS)\n",
    "        \n",
    "        if len(sel_substr)%NUM_ATOMS!=0:\n",
    "            raise Exception(len(sel_substr))\n",
    "\n",
    "        ### selector for matrix (remaining residues)\n",
    "        sel_matrix = np.arange(len(coords))\n",
    "        sel_matrix = np.delete(sel_matrix, sel_substr)\n",
    "\n",
    "        ### select coords and mask\n",
    "        coords_substr = coords[sel_substr]\n",
    "        coords_matrix = coords[sel_matrix]\n",
    "        mask_substr   = mask[sel_substr]\n",
    "        mask_matrix   = mask[sel_matrix]\n",
    "\n",
    "        outputs = [coords_substr, mask_substr]\n",
    "        \n",
    "        if return_contact_mask:\n",
    "            D = get_3D_tensor(coords_substr, coords_matrix) # [R1, R2, 64]\n",
    "            M = get_3D_tensor(mask_substr,   mask_matrix)   # [R1, R2, 64]\n",
    "\n",
    "            # Crucial!! This sets masked values to really large numbers!\n",
    "            # This is because we take the minimum later, but \"min\" does not know which values to ignore.\n",
    "            D[M==0] = 10**6 \n",
    "            D_min   = np.min(D, axis=2) # [R1, R2, 64] -> [R1, R2], use a single atom as a proxy for the whole residue\n",
    "\n",
    "            contact_mask = np.any(D_min<cutoff, axis=1)*1 # [R1]\n",
    "            outputs.append(contact_mask)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "def get_drift_path(pdb_id, model_idx, seed=None):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(100)\n",
    "    return '/sopran/shared/simrna_drift_dataset/20200327_drift_data/%s/model%02d/seed%04d.pkl' % (pdb_id, model_idx, seed)\n",
    "\n",
    "\n",
    "def load_drift_file(drift_path, wanted_rmsd=1.0):\n",
    "    if wanted_rmsd not in [1.0, 3.0, 5.0, 10]:\n",
    "        raise Exception('Invalid drift RMSD requested:', wanted_rmsd)\n",
    "        \n",
    "    with open(drift_path, 'rb') as fileobj:\n",
    "        return pickle.load(fileobj)[wanted_rmsd][0]\n",
    "    \n",
    "print('Done')\n",
    "\n",
    "#PARAMS['batch_size']*PARAMS['N_GPUS']  better use None\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.variable_scope('resnet', reuse=tf.AUTO_REUSE):\n",
    "        tconfig=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "        with tf.device('/cpu:' + str([x.name for x in device_lib.list_local_devices() if x.device_type == 'CPU'][0][-1])):\n",
    "\n",
    "            c1 = []\n",
    "            device_list = [x.name for x in device_lib.list_local_devices() if x.device_type == 'GPU']\n",
    "            device_list = ['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'] \n",
    "            print(\"device_list: %s \\n\" % device_list)\n",
    "            features_placeholder_full      = tf.placeholder(tf.float32, (None ,PARAMS['RNA_len'],PARAMS['RNA_len'] ,PARAMS['max_depth'] ))\n",
    "            features_mask_placeholder_full = tf.placeholder(tf.float32, (None, PARAMS['RNA_len'],PARAMS['RNA_len'] ,PARAMS['max_depth'] ))\n",
    "            train_ph                  = tf.placeholder(tf.bool, ())\n",
    "            seq_ph_full = tf.placeholder(tf.int32, [None])\n",
    "            step_ph  = tf.placeholder(tf.float32, ())\n",
    "        start_idx = 0\n",
    "        #if 1==1:\n",
    "        all_z  = []\n",
    "        all_xr = []\n",
    "        if 1==1:\n",
    "            #for d in device_list:\n",
    "            if 1==1:\n",
    "            #with tf.device(d):\n",
    "                features_placeholder      = features_placeholder_full[start_idx:start_idx + PARAMS['batch_size']]\n",
    "                features_mask_placeholder = features_mask_placeholder_full[start_idx:start_idx + PARAMS['batch_size']]\n",
    "                #train_ph                  = train_ph_full[start_idx:start_idx + PARAMS['batch_size']]\n",
    "                seq_ph                    = seq_ph_full[start_idx:start_idx + PARAMS['batch_size']]\n",
    "                #one_mask = tf.ones([seq_ph[0], seq_ph[0]])\n",
    "                one_list = []\n",
    "                for m in range(0,PARAMS['batch_size']) :\n",
    "                    newone_mask = tf.ones([seq_ph[m], seq_ph[m]])\n",
    "                    paddings = [[0, PARAMS['RNA_len']-tf.shape(newone_mask)[0]], [0, PARAMS['RNA_len']-tf.shape(newone_mask)[0]]]\n",
    "                    newone_mask = tf.pad(newone_mask, paddings, \"CONSTANT\")\n",
    "                    one_list.append(newone_mask)\n",
    "                    #one_mask = tf.concat([one_mask,newone_mask],axis=1)\n",
    "                one_mask = tf.stack(one_list)\n",
    "                one_exp_list = []\n",
    "                for m in range(0,embedding_dim) :\n",
    "                    one_exp_list.append(one_mask)\n",
    "                one_mask_tile = tf.stack(one_exp_list,-1)   \n",
    "                \n",
    "                # Build modules.\n",
    "                #encoder = Encoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "                #decoder = Decoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "\n",
    "                #pre_vq_conv1 = conv2d_layer(x, embedding_dim, \"to_vq\", 7)\n",
    "                encoder = Encoder(num_hiddens, embedding_dim, embedding_dim)\n",
    "                decoder = Decoder(num_hiddens, num_residual_layers, PARAMS['max_depth'])\n",
    "                # Process inputs with conv stack, finishing with 1x1 to get to correct size.\n",
    "                x = features_placeholder * features_mask_placeholder\n",
    "                #tf.placeholder(tf.float32, shape=(None, image_size, image_size, 3))\n",
    "                enc_x = encoder(x, train_ph) \n",
    "\n",
    "                if vq_use_ema:\n",
    "                    vq_vae = snt.nets.VectorQuantizerEMA(\n",
    "                          embedding_dim=embedding_dim,\n",
    "                          num_embeddings=num_embeddings,\n",
    "                          commitment_cost=commitment_cost,\n",
    "                          decay=decay)\n",
    "                else:\n",
    "                    vq_vae = snt.nets.VectorQuantizer(\n",
    "                          embedding_dim=embedding_dim,\n",
    "                          num_embeddings=num_embeddings,\n",
    "                          commitment_cost=commitment_cost)\n",
    "\n",
    "                #zp = pre_vq_conv1(enc_x) \n",
    "                #zp = enc_x#conv2d_layer(enc_x, embedding_dim, \"to_vq\", 3) \n",
    "                zp = conv2d_layer(enc_x, embedding_dim, \"to_vq\", 3) \n",
    "\n",
    "                z_T = tf.concat( [tf.stack([tf.reshape(tf.transpose(zp[i,:,:,ilast]),[1,PARAMS['RNA_len'],PARAMS['RNA_len']]) for ilast in range(embedding_dim)], axis=-1) for i in range(batch_size)] , axis=0) \n",
    "                z  = 0.5 * (zp + z_T) * one_mask_tile #tf.transpose(zp, [0,1,2,3]))\n",
    "\n",
    "                # For training\n",
    "                vq_output_train = vq_vae(z, is_training=True)\n",
    "\n",
    "                #x_recon_pre = decoder(vq_output_train[\"quantize\"])\n",
    "                dec_in = vq_output_train[\"quantize\"] * one_mask_tile\n",
    "                x_recon = decoder(dec_in, features_mask_placeholder) * features_mask_placeholder\n",
    "                # For evaluation, make sure is_training=False!\n",
    "                vq_output_eval = vq_vae(z, is_training=False)\n",
    "                #vq_output_eval_tensor  = vq_output_eval[\"quantize\"]\n",
    "                #vq_output_eval_tensor += tf.transpose(vq_output_eval_tensor,[0,1,2,3])\n",
    "                #vq_output_eval_tensor *= 0.5\n",
    "                xrec = x_recon_eval = decoder(vq_output_eval[\"quantize\"], features_mask_placeholder)\n",
    "                zenc = tf.reshape(vq_output_eval['encodings'],[PARAMS[\"batch_size\"],100,100,3])\n",
    "                recon_error = tf.reduce_mean((x_recon - x)**2 * features_mask_placeholder) \n",
    "                loss = recon_error + vq_output_train[\"loss\"] \n",
    "                c1.append(loss)\n",
    "                start_idx = start_idx + PARAMS[\"batch_size\"]\n",
    "                all_z.append( zenc )\n",
    "                all_xr.append(x_recon_eval)\n",
    "        zs = tf.concat(all_z,0)\n",
    "        xr = tf.concat(x_recon_eval,0)\n",
    "        with tf.device('/cpu:' + str([x.name for x in device_lib.list_local_devices() if x.device_type == 'CPU'][0][-1])):\n",
    "            tot_loss = tf.add_n(c1) / PARAMS['N_GPUS']\n",
    "            \n",
    "\n",
    "\n",
    "            #x_recon_evalp_T = tf.concat( [tf.stack([tf.reshape(tf.transpose(x_recon_evalp[i,:,:,ilast]),[1,PARAMS['RNA_len'],PARAMS['RNA_len']])\\\n",
    "            #                                   for ilast in range(PARAMS['max_depth'])], axis=-1) for i in range(batch_size)] , axis=0) \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            #x_recon_eval =  0.5 * (x_recon_evalp + x_recon_evalp_T)\n",
    "\n",
    "            # The following is a useful value to track during training.\n",
    "            # It indicates how many codes are 'active' on average.\n",
    "            perplexity = vq_output_train[\"perplexity\"] \n",
    "\n",
    "            # Create optimizer and TF session.\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            #train_op = optimizer.minimize(tot_loss)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            global_step = tf.train.get_global_step()\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            gradients = optimizer.compute_gradients(tot_loss, tvars, colocate_gradients_with_ops=True)\n",
    "            minimize_op = optimizer.apply_gradients(gradients, global_step=global_step, name=\"train\")\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            train_op = tf.group(minimize_op, update_ops)\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver( max_to_keep=0)\n",
    "            init_op = tf.global_variables_initializer()\n",
    "print('Done setting up graph.')\n",
    "\n",
    "\n",
    "# # Dataset lodaer\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "### Longer sequences should be shown more often (b/c of data augmentation)\n",
    "#when want to sample remaining_ids_sampled ***1\n",
    "lengths       = np.array(list(traindata_df['len']))\n",
    "probabilities = lengths #/ np.sum(lengths)\n",
    "\n",
    "### Sequences in larger clusters should be shown less often\n",
    "cluster_sizes = np.array(list(traindata_df['cluster_size']))\n",
    "model_sizes = np.array(list(traindata_df['num_models']))\n",
    "probabilities = probabilities/cluster_sizes/model_sizes\n",
    "probabilities = probabilities/np.sum(probabilities)\n",
    "\n",
    "### Draw sample\n",
    "pdb_ids = np.array(list(traindata_df.index))\n",
    "sampled_pdb_ids = np.random.choice(pdb_ids, 50, p=probabilities, replace= False)\n",
    "#sampled_pdb_ids = list(set(sampled_pdb_ids))[:50]\n",
    "#print(sampled_pdb_ids)\n",
    "print(len(sampled_pdb_ids))\n",
    "\n",
    "# VALIDATION\n",
    "\n",
    "### Longer sequences should be shown more often (b/c of data augmentation)\n",
    "#when want to sample remaining_ids_sampled ***1\n",
    "lengths       = np.array(list(valdata_df['len']))\n",
    "probabilities_val = lengths #/ np.sum(lengths)\n",
    "\n",
    "### Sequences in larger clusters should be shown less often\n",
    "cluster_sizes = np.array(list(valdata_df['cluster_size']))\n",
    "model_sizes = np.array(list(valdata_df['num_models']))\n",
    "probabilities_val = probabilities_val/cluster_sizes/model_sizes\n",
    "probabilities_val = probabilities_val/np.sum(probabilities_val)\n",
    "\n",
    "### Draw sample\n",
    "pdb_ids_val = np.array(list(valdata_df.index))\n",
    "sampled_pdb_ids_val = np.random.choice(pdb_ids_val, 50, p=probabilities_val, replace= False)\n",
    "#sampled_pdb_ids = list(set(sampled_pdb_ids))[:50]\n",
    "#print(sampled_pdb_ids)\n",
    "print(len(sampled_pdb_ids_val))\n",
    "\n",
    "# TEST\n",
    "\n",
    "### Longer sequences should be shown more often (b/c of data augmentation)\n",
    "#when want to sample remaining_ids_sampled ***1\n",
    "lengths       = np.array(list(testdata_df['len']))\n",
    "probabilities_test = lengths #/ np.sum(lengths)\n",
    "\n",
    "### Sequences in larger clusters should be shown less often\n",
    "cluster_sizes = np.array(list(testdata_df['cluster_size']))\n",
    "model_sizes = np.array(list(testdata_df['num_models']))\n",
    "probabilities_test = probabilities_test/cluster_sizes/model_sizes\n",
    "probabilities_test = probabilities_test/np.sum(probabilities_test)\n",
    "\n",
    "### Draw sample\n",
    "pdb_ids_test = np.array(list(testdata_df.index))\n",
    "#sampled_pdb_ids_test = np.random.choice(pdb_ids_test, 50, p=probabilities_test, replace= False)\n",
    "#sampled_pdb_ids = list(set(sampled_pdb_ids))[:50]\n",
    "#print(sampled_pdb_ids)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "\n",
    "def batch_data_train( batch_size = PARAMS['batch_size']*PARAMS['N_GPUS'], trainflag='train'):\n",
    "    if trainflag == 'train':\n",
    "        sampled_pdb_ids = np.random.choice(pdb_ids, batch_size, replace= False)\n",
    "        return traindata_df.loc[sampled_pdb_ids]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in traindata_df.loc[sampled_pdb_ids]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in traindata_df.loc[sampled_pdb_ids]['mask'].values])\n",
    "    \n",
    "    if trainflag == 'valid':\n",
    "        sampled_pdb_ids_val = np.random.choice(pdb_ids_val, batch_size, replace= False)\n",
    "        return valdata_df.loc[sampled_pdb_ids_val]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in valdata_df.loc[sampled_pdb_ids_val]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in valdata_df.loc[sampled_pdb_ids_val]['mask'].values])       \n",
    "                                           \n",
    "    if trainflag == 'test':\n",
    "        sampled_pdb_ids_test = np.random.choice(pdb_ids_test, batch_size, replace= False)\n",
    "        return testdata_df.loc[sampled_pdb_ids_test]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in testdata_df.loc[sampled_pdb_ids_test]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in testdata_df.loc[sampled_pdb_ids_test]['mask'].values])        \n",
    "\n",
    "\n",
    "def iter_data_train( i, batch_size = PARAMS['batch_size']*PARAMS['N_GPUS'], trainflag='train'):\n",
    "    if trainflag == 'train':\n",
    "        sampled_pdb_ids = pdb_ids[i*batch_size:(i+1)*batch_size]#np.random.choice(pdb_ids, batch_size, replace= False)\n",
    "        if len(sampled_pdb_ids)<batch_size:\n",
    "            sampled_pdb_ids = pdb_ids[-batch_size:]\n",
    "        return sampled_pdb_ids,traindata_df.loc[sampled_pdb_ids]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in traindata_df.loc[sampled_pdb_ids]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in traindata_df.loc[sampled_pdb_ids]['mask'].values])\n",
    "    \n",
    "    if trainflag == 'valid':\n",
    "        sampled_pdb_ids_val = pdb_ids_val[i*batch_size:(i+1)*batch_size]#np.random.choice(pdb_ids_val, batch_size, replace= False)\n",
    "        if len(sampled_pdb_ids_val)<batch_size:\n",
    "            sampled_pdb_ids_val = pdb_ids_val[-batch_size:]\n",
    "        return sampled_pdb_ids_val,valdata_df.loc[sampled_pdb_ids_val]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in valdata_df.loc[sampled_pdb_ids_val]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in valdata_df.loc[sampled_pdb_ids_val]['mask'].values])       \n",
    "                                           \n",
    "    if trainflag == 'test':\n",
    "        sampled_pdb_ids_test = pdb_ids_test[i*batch_size:(i+1)*batch_size]#np.random.choice(pdb_ids_test, batch_size, replace= False)\n",
    "        if len(sampled_pdb_ids_test)<batch_size:\n",
    "            sampled_pdb_ids_test = pdb_ids_test[-batch_size:]\n",
    "        return sampled_pdb_ids_test,testdata_df.loc[sampled_pdb_ids_test]['len'].values,            \n",
    "    np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in testdata_df.loc[sampled_pdb_ids_test]['coords'].values]),\n",
    "    np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in testdata_df.loc[sampled_pdb_ids_test]['mask'].values])        \n",
    "\n",
    "print('ckpt dir: ', head_ckpt_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open( '/gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/L64x64_substructure_23022022.pkl', 'rb') as f:\n",
    "#    traindata_df, valdata_df, testdata_df = pickle.load( f)\n",
    "#print(len(traindata_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_ids = np.array(list(traindata_df.index))\n",
    "pdb_ids_val = np.array(list(valdata_df.index))\n",
    "pdb_ids_test = np.array(list(testdata_df.index))\n",
    "def pdb_batch_data_train( batch_size = PARAMS['batch_size']*PARAMS['N_GPUS'], trainflag='train'):\n",
    "    if trainflag == 'train':\n",
    "        sampled_pdb_ids = np.random.choice(pdb_ids, batch_size, replace= False)\n",
    "        return sampled_pdb_ids, traindata_df.loc[sampled_pdb_ids]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in traindata_df.loc[sampled_pdb_ids]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in traindata_df.loc[sampled_pdb_ids]['mask'].values])\n",
    "    \n",
    "    if trainflag == 'valid':\n",
    "        sampled_pdb_ids_val = np.random.choice(pdb_ids_val, batch_size, replace= False)\n",
    "        return sampled_pdb_ids_val,  valdata_df.loc[sampled_pdb_ids_val]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in valdata_df.loc[sampled_pdb_ids_val]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in valdata_df.loc[sampled_pdb_ids_val]['mask'].values])       \n",
    "                                           \n",
    "    if trainflag == 'test':\n",
    "        sampled_pdb_ids_test = np.random.choice(pdb_ids_test, batch_size, replace= False)\n",
    "        return sampled_pdb_ids_test, testdata_df.loc[sampled_pdb_ids_test]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in testdata_df.loc[sampled_pdb_ids_test]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in testdata_df.loc[sampled_pdb_ids_test]['mask'].values])        \n",
    "\n",
    "def iter_data_train( i, batch_size = PARAMS['batch_size']*PARAMS['N_GPUS'], trainflag='train'):\n",
    "    if trainflag == 'train':\n",
    "        sampled_pdb_ids = pdb_ids[i*batch_size:(i+1)*batch_size]#np.random.choice(pdb_ids, batch_size, replace= False)\n",
    "        if len(sampled_pdb_ids)<batch_size:\n",
    "            sampled_pdb_ids = pdb_ids[-batch_size:]\n",
    "        return sampled_pdb_ids,traindata_df.loc[sampled_pdb_ids]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in traindata_df.loc[sampled_pdb_ids]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in traindata_df.loc[sampled_pdb_ids]['mask'].values])\n",
    "    \n",
    "    if trainflag == 'valid':\n",
    "        sampled_pdb_ids_val = pdb_ids_val[i*batch_size:(i+1)*batch_size]#np.random.choice(pdb_ids_val, batch_size, replace= False)\n",
    "        if len(sampled_pdb_ids_val)<batch_size:\n",
    "            sampled_pdb_ids_val = pdb_ids_val[-batch_size:]\n",
    "        return sampled_pdb_ids_val,valdata_df.loc[sampled_pdb_ids_val]['len'].values,             np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in valdata_df.loc[sampled_pdb_ids_val]['coords'].values]),             np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in valdata_df.loc[sampled_pdb_ids_val]['mask'].values])       \n",
    "                                           \n",
    "    if trainflag == 'test':\n",
    "        sampled_pdb_ids_test = pdb_ids_test[i*batch_size:(i+1)*batch_size]#np.random.choice(pdb_ids_test, batch_size, replace= False)\n",
    "        if len(sampled_pdb_ids_test)<batch_size:\n",
    "            sampled_pdb_ids_test = pdb_ids_test[-batch_size:]\n",
    "        return sampled_pdb_ids_test,testdata_df.loc[sampled_pdb_ids_test]['len'].values,  np.stack([np_pad(get_3D_tensor(tcoord)) for tcoord in testdata_df.loc[sampled_pdb_ids_test]['coords'].values]),np.stack([np_pad_mask(get_3D_tensor(tcoord)) for tcoord in testdata_df.loc[sampled_pdb_ids_test]['mask'].values])        \n",
    "\n",
    "    \n",
    "def get_max_dist_neighbour(pdb_weirdo, flag = 'train'):\n",
    "    if flag == 'train':\n",
    "        weirdo = get_3D_tensor(traindata_df.loc[pdb_weirdo]['coords'])[:,:,0]\n",
    "    elif flag == 'valid':\n",
    "        weirdo = get_3D_tensor(valdata_df.loc[pdb_weirdo]['coords'])[:,:,0]\n",
    "    elif flag == 'test':\n",
    "        weirdo = get_3D_tensor(testdata_df.loc[pdb_weirdo]['coords'])[:,:,0]\n",
    "    check = np.zeros(weirdo.shape) \n",
    "    check[10:,10:] += weirdo[10:,10:]\n",
    "    cut_matrix = weirdo - check\n",
    "    cut = cut_matrix.max()\n",
    "    check = np.zeros(weirdo.shape) \n",
    "    #check[2:,10:15] += weirdo[2:,10:15]\n",
    "    check[5:,5:] += weirdo[5:,5:]\n",
    "    beta = weirdo - check\n",
    "    beta = beta[4].max()\n",
    "    \n",
    "    diag_shift = weirdo.diagonal()[1:] + weirdo[1:,:-1].diagonal()\n",
    "    \n",
    "    return cut, beta, diag_shift.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cp /gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/vqvae100_3d_8/ckpt/plots/vqvae_01022022_loss_L100_lre5_emb3_dim8.png ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -ltr /gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/vqvae100_3d_8/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##### SINGLE MODE #####\\nhead_ckpt_dir = head_sopran_dir = \\'/gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/vqvae100_3d_8/ckpt/\\'\\nz_data = {}\\n\\nwith open(\\'./data/puzzle.pkl\\', \\'rb\\') as f:\\n    puzzle, idx_in = pickle.load(f)\\n    \\nprint(puzzle)\\npuzzle_seq = np.array([enc_seq_pdb.encode(puzzle) for i in range(PARAMS[\\'batch_size\\'])])\\nx_in = np.ones((PARAMS[\\'batch_size\\'], 100, 100, 64))\\nmask_in = np.ones((PARAMS[\\'batch_size\\'], 100, 100, 64))\\nlseq_in = np.array([len(puzzle) for i in range(PARAMS[\\'batch_size\\']) ])\\n\\n\\n    \\n#head_ckpt_dir = \\'/gpfs/scratch/ramakers/sopran/shared/embeddings/vqvae_3d_100_0810/tmp/\\'\\n#head_sopran_dir = \\'/gpfs/scratch/ramakers/sopran/vqvae/\\'\\nn_iter = 160000\\nbatch_size = 1\\nplotme = False\\nfull_res = []        \\nwith tf.Session(graph=g, config=tconfig) as sess:\\n    saver.restore(sess, head_ckpt_dir + \"model_L50_15122020_L100_lre5_emb3_dim8_3_train_{0}.ckpt\".format(n_iter))\\n    feed_dict = {vq_output_eval[\\'encoding_indices\\']: idx_in,features_placeholder_full : x_in,  features_mask_placeholder_full : mask_in , train_ph: False, seq_ph_full:lseq_in }\\n    results = sess.run(x_recon_eval, feed_dict=feed_dict)\\n\\nz_data.update({puzzle : [ idx_in[0] , results[0]]})    \\nplt.imshow(results[0,:,:,0])\\nplt.show()'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"##### SINGLE MODE #####\n",
    "head_ckpt_dir = head_sopran_dir = '/gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/vqvae100_3d_8/ckpt/'\n",
    "z_data = {}\n",
    "\n",
    "with open('./data/puzzle.pkl', 'rb') as f:\n",
    "    puzzle, idx_in = pickle.load(f)\n",
    "    \n",
    "print(puzzle)\n",
    "puzzle_seq = np.array([enc_seq_pdb.encode(puzzle) for i in range(PARAMS['batch_size'])])\n",
    "x_in = np.ones((PARAMS['batch_size'], 100, 100, 64))\n",
    "mask_in = np.ones((PARAMS['batch_size'], 100, 100, 64))\n",
    "lseq_in = np.array([len(puzzle) for i in range(PARAMS['batch_size']) ])\n",
    "\n",
    "\n",
    "    \n",
    "#head_ckpt_dir = '/gpfs/scratch/ramakers/sopran/shared/embeddings/vqvae_3d_100_0810/tmp/'\n",
    "#head_sopran_dir = '/gpfs/scratch/ramakers/sopran/vqvae/'\n",
    "n_iter = 160000\n",
    "batch_size = 1\n",
    "plotme = False\n",
    "full_res = []        \n",
    "with tf.Session(graph=g, config=tconfig) as sess:\n",
    "    saver.restore(sess, head_ckpt_dir + \"model_L50_15122020_L100_lre5_emb3_dim8_3_train_{0}.ckpt\".format(n_iter))\n",
    "    feed_dict = {vq_output_eval['encoding_indices']: idx_in,features_placeholder_full : x_in,  features_mask_placeholder_full : mask_in , train_ph: False, seq_ph_full:lseq_in }\n",
    "    results = sess.run(x_recon_eval, feed_dict=feed_dict)\n",
    "\n",
    "z_data.update({puzzle : [ idx_in[0] , results[0]]})    \n",
    "plt.imshow(results[0,:,:,0])\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /software/tensorflow/1.14/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/vqvae100_3d_8/ckpt/model_L50_15122020_L100_lre5_emb3_dim8_3_train_160000.ckpt\n",
      "3owz_A GGCUCUGGAGAGAACCGUUUAAUCGGUCGCCGAAGGAGCAAGCUCUGCGGAAACGCAGAGUGAAACUCUCAGGCAAAAGGACAGAGUC\n",
      "5lyu_B GGGGAUCUGUCACCCCAUUGAUCGCCUUCGGGCUGAUCUGGCUGGCUAGGCGGGUCCC\n",
      "6jq5_B UUACUGUGAGAAUCAGUAACAAACAUGUGGGGCUUAUAUCUAAUCGAAAGAUUAGUAUUAGUGCAGACGUUAAAACCAUGUC\n",
      "5di4_A GGGUACUUAAGCCCACUGAUGAGUCGCUGGGAUGCGACGAAACGCCCA\n",
      "5swd_A GGGAAGAUAUAAUCCUAAUGAUAUGGUUUGGGAGUUUCUACCAAGAGCCUUAAACUCUUGAUUAUCUUCCC\n",
      "4qlm_A CGCUGAACCCGAAAGGGGCGGGGGACCCAGAAAUGGGGCGAAUCUCUUCCGAAAGGAAGAGUAGGGUUACUCCUUCGACCCGAGCCCGUCAGCUAACCUC\n",
      "1y26_X CGCUUCAUAUAAUCCUAAUGAUAUGGUUUGGGAGUUUCUACCAAGAGCCUUAAACUCUUGAUUAUGAAGUG\n",
      "5kpy_A GGACACUGAUGAUCGCGUGGAUAUGGCACGCAUUGAAUUGUUGGACACCGUAAAUGUCCUAACACGUGUCC\n",
      "6pmo_A GAAAGUGGGUGCGCGUUUGGCGCAUCAACUCGGGUGGAACCGCGGGAGCUACGCUCUCGUCCCGAG\n",
      "6pom_A GAUGAGCACGCAACGAAAGGCAUUCUUGAGCAAUUUUAAAAAAGAGGCUGGGAUUUUGUUCUCAGCAACUAGGGUGGAACCGCGGGAGAACUCUCGUCCC\n",
      "6ufm_B GGCGACGAUCCGGCCAUCACCGGGGAGCCUUCGGAAGAACGGCGCCGCCGGAAACGGCGGCGCUCAGUAGAACCGAACGGGUGAGCCCGUCACAGCUC\n",
      "4lck_C GUGCGAUGAGAAGAAGAGUAUUAAGGAUUUACUAUGAUUAGCGACUCUAGGAUAGUGAAAGCUAGAGGAUAGUAACCUUAAGAAGGCACUUCGAGCACCC\n",
      "4r4v_A CGGCCCAAGCGGUAGUAAGCAGGGAACUCACCUCCAAUGAAACACAUUGUCGUAGCAGUUGACUACUGUUAUGUGAUUGGUAGAGGCUAAGUGACGGUAU\n",
      "5k7c_B AUCAGGUGCAA\n",
      "4p9r_A GGGUUGGGUUGGGAAGUAUCAUGGCUAAUCACCAUGAUGCAAUCGGGUUGAACACUUAAUUGGGUUAAAACGGUGGGGGACGAUCCCGUAACAUCCGUCC\n",
      "4lck_E GAGUAGUUCAGUGGUAGAACACCACCUUGCCAAGGUGGGGGUCGCGGGUUCGAAUCCCGUCUCGGGCGAAAGCCC\n",
      "5nwq_B CCGGACGAGGUGCGCCGUACCCGGUCAGGACAAGACGGCGC\n",
      "6pom_B GCGGAAGUAGUUCAGUGGUAGAACACCACCUUGCCAAGGUGGGGGUCGCGGGUUCGAAUCCCGUCUUCCGCUCCA\n",
      "6pmo_B GCGGAAGUAGUUCAGUGGUAGAACACCACCUUGCCAAGGUGGGGGUCGCGGGUUCGAAUCCCGUCUUCCGCUCCA\n",
      "5y85_A ACCCGCAAGGCCGACGGC\n",
      "5y85_C ACCCGCAAGGCCGACGGC\n",
      "5y85_D GCCGCCGCUGGUGCAAGUCCAGCCACGCUUCGGCGUGGGCGCUCAUGGGU\n",
      "3owz_B GGCUCUGGAGAGAACCGUUUAAUCGGUCGCCGAAGGAGCAAGCUCUGCGGAAACGCAGAGUGAAACUCUCAGGCAAAAGGACAGAGUC\n",
      "5lyu_A GGGGAUCUGUCACCCCAUUGAUCGCCUUCGGGCUGAUCUGGCUGGCUAGGCGGGUCCC\n",
      "4l81_A GGAUCACGAGGGGGAGACCCCGGCAACCUGGGACGGACACCCAAGGUGCUCACACCGGAGACGGUGGAUCCGGCCCGAGAGGGCAACGAAGUCCGU\n",
      "5k7c_A CGUGGUUAGGGCCACGUUAAAUAGUUGCUUAAGCCCUAAGCGUUGAU\n",
      "5tpy_A GGGUCAGGCCGGCGAAAGUCGCCACAGUUUGGGGAAAGCUGUGCAGCCUGUAACCCCCCCACGAAAGUGGG\n",
      "5nwq_A CCGGACGAGGUGCGCCGUACCCGGUCAGGACAAGACGGCGC\n",
      "4lck_F GGGUGCGAUGAGAAGAAGAGUAUUAAGGAUUUACUAUGAUUAGCGACUCUAGGAUAGUGAAAGCUAGAGGAUAGUAACCUUAAGAAGGCACUUCGAGCAC\n",
      "6jq5_A UUACUGUGAGAAUCAGUAACAAACAUGUGGGGCUUAUAUCUAAUCGAAAGAUUAGUAUUAGUGCAGACGUUAAAACCAUGUC\n",
      "5swd_B GGGAAGAUAUAAUCCUAAUGAUAUGGUUUGGGAGUUUCUACCAAGAGCCUUAAACUCUUGAUUAUCUUCCC\n",
      "5di4_B GGGCGUCUGGGCAGUACCCA\n",
      "6ufm_A GGGCCUAUAGCUCAGGCGGUUAGAGCGCUUCGCUGAUAACGAAGAGGUCGGAGGUUCGAGUCCUCCUAGGCCCACCA\n",
      "6ol3_C CUCGCAAGGGUAUCAUGGCGGACGACCGGGGUUCGAACCCCGGAUCCGGCCGUCCGCCGUGAUCCAUGCGGUUACCGCCCGCGUGUCGAACCCAGGUGUG\n",
      "5y85_B GCCGCCGCUGGUGCAAGUCCAGCCACGCUUCGGCGUGGGCGCUCAUGGGU\n",
      "5swe_X GGGAAGAUAUAAUCCUAAUGAUAUGGUUUGGGAGUUUCUACCAAGAGCCUUAAACUCUUGAUUAUCUUCCC\n",
      "4lck_B GAGUAGUUCAGUGGUAGAACACCACCUUGCCAAGGUGGGGGUCGCGGGUUCGAAUCCCGUCUCGGGCGAAAGCCC\n",
      "6p2h_A GGGUGUAAUCUCCAAAAUAUGGUUGGGGAGCCUCCACCAGUGAACCGUAAAAUCGCUGUCACCACCCAG\n",
      "4xw7_A GGGUCGUGACUGGCGAACAGGUGGGAAACCACCGGGGAGCGACCCUUGCCGCCCGCCUGGGCAA\n"
     ]
    }
   ],
   "source": [
    "##### MULTI PUZZLE MODE #####\n",
    "head_ckpt_dir = head_sopran_dir = '/gpfs/scratch/ramakers/rnafold/data/Lx64x64/31012022/vqvae/vqvae100_3d_8/ckpt/'\n",
    "z_data = {}\n",
    "# att4_zero\n",
    "#for idx in [1900000, 2000000, 2100000, 2200000, 2300000, 2400000]: noshape_zero\n",
    "#[1100000, 1200000, 1300000, 1400000,1500000, 1600000] 1975000\n",
    "idx =    2035000\n",
    "#with open('./data/att4_zero_{0}.pkl'.format(idx), 'rb') as f:\n",
    "with open('./data/noshape_zero.pkl'.format(idx), 'rb') as f:\n",
    "    puzzles = pickle.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#head_ckpt_dir = '/gpfs/scratch/ramakers/sopran/shared/embeddings/vqvae_3d_100_0810/tmp/'\n",
    "#head_sopran_dir = '/gpfs/scratch/ramakers/sopran/vqvae/'\n",
    "n_iter = 160000\n",
    "batch_size = 1\n",
    "plotme = False\n",
    "full_res = []        \n",
    "with tf.Session(graph=g, config=tconfig) as sess:\n",
    "    saver.restore(sess, head_ckpt_dir + \"model_L50_15122020_L100_lre5_emb3_dim8_3_train_{0}.ckpt\".format(n_iter))\n",
    "    for pdb in list(puzzles.keys()):\n",
    "        puzzle, idx_in = puzzles[pdb]\n",
    "        print(pdb, puzzle)\n",
    "        puzzle_seq = np.array([enc_seq_pdb.encode(puzzle) for i in range(PARAMS['batch_size'])])\n",
    "        x_in = np.ones((PARAMS['batch_size'], 100, 100, 64))\n",
    "        mask_in = np.ones((PARAMS['batch_size'], 100, 100, 64))\n",
    "        lseq_in = np.array([len(puzzle) for i in range(PARAMS['batch_size']) ])\n",
    "\n",
    "        feed_dict = {vq_output_eval['encoding_indices']: idx_in,features_placeholder_full : x_in,  features_mask_placeholder_full : mask_in , train_ph: False, seq_ph_full:lseq_in }\n",
    "        results = sess.run(x_recon_eval, feed_dict=feed_dict)\n",
    "\n",
    "        z_data.update({pdb : [puzzle, idx_in[0] , results[0]]})    \n",
    "#plt.imshow(results[0,:,:,0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode and relax into into Lx8 structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3owz_A 88 10.582450699008172\n",
      "\n",
      "########## 160000 10.582450699008172 0.0\n",
      "\n",
      "\n",
      "5lyu_B 58 12.36498266602876\n",
      "\n",
      "########## 160000 11.473716682518466 0.8912659835102943\n",
      "\n",
      "\n",
      "6jq5_B 82 13.313753217673874\n",
      "\n",
      "########## 160000 12.087062194236935 1.1322347234851216\n",
      "\n",
      "\n",
      "5di4_A 48 11.542161446408176\n",
      "\n",
      "########## 160000 11.950837007279745 1.0085328484901404\n",
      "\n",
      "\n",
      "5swd_A 71 7.5922839793522465\n",
      "\n",
      "########## 160000 11.079126401694245 1.9629641679734922\n",
      "\n",
      "\n",
      "4qlm_A 100 21.5153876081236\n",
      "\n",
      "########## 160000 12.81850326943247 4.2823104796442255\n",
      "\n",
      "\n",
      "1y26_X 71 7.984748579006682\n",
      "\n",
      "########## 160000 12.127966885085929 4.31039274287644\n",
      "\n",
      "\n",
      "5kpy_A 71 6.927348986797687\n",
      "\n",
      "########## 160000 11.477889647799898 4.383520594131355\n",
      "\n",
      "\n",
      "6pmo_A 66 20.280986883009533\n",
      "\n",
      "########## 160000 12.45601156282319 4.9733293230558635\n",
      "\n",
      "\n",
      "6pom_A 100 37.73745875295554\n",
      "\n",
      "########## 160000 14.984156281836425 8.932202728469676\n",
      "\n",
      "\n",
      "6ufm_B 98 18.426298779513456\n",
      "\n",
      "########## 160000 15.29707832707979 8.57381679368664\n",
      "\n",
      "\n",
      "4lck_C 100 16.07244562681452\n",
      "\n",
      "########## 160000 15.361692268724353 8.211601414263304\n",
      "\n",
      "\n",
      "4r4v_A 100 10.879574389416332\n",
      "\n",
      "########## 160000 15.016913970316041 7.979342642816428\n",
      "\n",
      "\n",
      "5k7c_B 11 9.07612369506412\n",
      "\n",
      "########## 160000 14.592571807798048 7.839829072713776\n",
      "\n",
      "\n",
      "4p9r_A 100 5.0412913049009935\n",
      "\n",
      "########## 160000 13.955819774271578 7.939882620514295\n",
      "\n",
      "\n",
      "4lck_E 75 11.966706093568341\n",
      "\n",
      "########## 160000 13.831500169227628 7.702821430410914\n",
      "\n",
      "\n",
      "5nwq_B 41 11.715681738309678\n",
      "\n",
      "########## 160000 13.707040261526572 7.489399099773705\n",
      "\n",
      "\n",
      "6pom_B 75 10.779331184685857\n",
      "\n",
      "########## 160000 13.544389757257642 7.309217802685353\n",
      "\n",
      "\n",
      "6pmo_B 75 11.924066207295484\n",
      "\n",
      "########## 160000 13.45910957041753 7.123464691016969\n",
      "\n",
      "\n",
      "5y85_A 18 8.894526404274313\n",
      "\n",
      "########## 160000 13.23088041211037 7.014003433794963\n",
      "\n",
      "\n",
      "5y85_C 18 9.099253461773293\n",
      "\n",
      "########## 160000 13.034136271618127 6.901284736327907\n",
      "\n",
      "\n",
      "5y85_D 50 9.158372974856453\n",
      "\n",
      "########## 160000 12.857965212674413 6.790772800638588\n",
      "\n",
      "\n",
      "3owz_B 88 12.533378840732636\n",
      "\n",
      "########## 160000 12.843852761720422 6.641836671928602\n",
      "\n",
      "\n",
      "5lyu_A 58 12.62259813062448\n",
      "\n",
      "########## 160000 12.834633818758093 6.502143186862813\n",
      "\n",
      "\n",
      "4l81_A 96 6.828298132862591\n",
      "\n",
      "########## 160000 12.594380391322272 6.478585679532196\n",
      "\n",
      "\n",
      "5k7c_A 47 9.55754348446956\n",
      "\n",
      "########## 160000 12.477578971827937 6.379563168410758\n",
      "\n",
      "\n",
      "5tpy_A 71 9.170128167252566\n",
      "\n",
      "########## 160000 12.3550807938807 6.2913919905922855\n",
      "\n",
      "\n",
      "5nwq_A 41 11.719435995866608\n",
      "\n",
      "########## 160000 12.332379193951626 6.179150340332681\n",
      "\n",
      "\n",
      "4lck_F 100 22.14675954829328\n",
      "\n",
      "########## 160000 12.670806102722029 6.330260607292693\n",
      "\n",
      "\n",
      "6jq5_A 82 16.38991300391565\n",
      "\n",
      "########## 160000 12.794776332761815 6.259564625263301\n",
      "\n",
      "\n",
      "5swd_B 71 8.114542005836222\n",
      "\n",
      "########## 160000 12.643801031893247 6.213052007032273\n",
      "\n",
      "\n",
      "5di4_B 20 18.400843836579405\n",
      "\n",
      "########## 160000 12.82370861953969 6.1966984119487485\n",
      "\n",
      "\n",
      "6ufm_A 77 10.651617567376105\n",
      "\n",
      "########## 160000 12.757887678565035 6.113435995483187\n",
      "\n",
      "\n",
      "6ol3_C 100 16.995338493287672\n",
      "\n",
      "########## 160000 12.882518584880406 6.065265527407056\n",
      "\n",
      "\n",
      "5y85_B 50 9.125228932814947\n",
      "\n",
      "########## 160000 12.77516745196425 6.010673928078892\n",
      "\n",
      "\n",
      "5swe_X 71 6.561545190216962\n",
      "\n",
      "########## 160000 12.602566833582381 6.013927502070989\n",
      "\n",
      "\n",
      "4lck_B 75 11.932713822534696\n",
      "\n",
      "########## 160000 12.584462698148661 5.933095996765177\n",
      "\n",
      "\n",
      "6p2h_A 69 10.373024084962172\n",
      "\n",
      "########## 160000 12.52626694517007 5.865200683867198\n",
      "\n",
      "\n",
      "4xw7_A 64 9.793793136426984\n",
      "\n",
      "########## 160000 12.456203514176657 5.805605096630158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_n = {}\n",
    "\n",
    "res = {}\n",
    "\n",
    "for pdb in list(z_data.keys()):\n",
    "\n",
    "    #print('Puzzle: ', pdb)\n",
    "    tseq = testdata_df.loc[pdb]['subseq']\n",
    "    tlen = len(tseq)\n",
    "\n",
    "    LxLx64_distance_tensor     = z_data[pdb][-1][:tlen,:tlen]\n",
    "    LxLx64_distance_truetensor = z_data[pdb][-1][:tlen,:tlen]\n",
    "\n",
    "    Lx3_coords = testdata_df.loc[pdb]['coords']\n",
    "    L_mask     = testdata_df.loc[pdb]['mask']\n",
    "\n",
    "    L8xL8_distance_matrix = LxLx64_tensor_to_L8xL8_matrix(symmetrize_LxLx64(LxLx64_distance_tensor))\n",
    "\n",
    "    Lx3_coords_reconstructed = L8xL8_matrix_to_Lx3_coords(L8xL8_distance_matrix, L_mask)\n",
    "\n",
    "    rms,ref_struct, recon_struct = superimposer(Lx3_coords, Lx3_coords_reconstructed, L_mask)\n",
    "    res.update({pdb:rms})\n",
    "    \n",
    "    print()\n",
    "    print(pdb, tlen, rms)\n",
    "    print()\n",
    "    \"\"\"\n",
    "    xsize, ysize = 10, 5\n",
    "    print('preds softmax')\n",
    "    fig, ax = plt.subplots(1,3, figsize=(xsize,ysize))\n",
    "    ax[0].imshow(z_data[pdb][1])\n",
    "    ax[1].imshow(z_data[pdb][-1][:,:,0])\n",
    "    ax[2].imshow(get_3D_tensor(Lx3_coords)[:,:,0])\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=[10,10])\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot(ref_struct[:,0],ref_struct[:,1],ref_struct[:,2], 'b-o', alpha=0.5)\n",
    "    ax.plot(recon_struct[:,0],recon_struct[:,1],recon_struct[:,2], 'r:o', alpha=0.5) \n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    res_n.update({n_iter:res.copy()})\n",
    "\n",
    "    print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "    print()\n",
    "\n",
    "    #occupancy = np.ones(len(Lx3_coords_reconstructed))\n",
    "    #temperature_factor = np.zeros(len(Lx3_coords_reconstructed)) \n",
    "    #DO_WRITE = write_8_atom_structure_to_pdb_file('./pdbs/'+str(pdb)+'.pdb', Lx3_coords_reconstructed, L_mask, tseq, occupancy, temperature_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1y26_X \t5.90\n",
      "3owz_A \t8.10\n",
      "3owz_B \t12.89\n",
      "4l81_A \t6.04\n",
      "4lck_B \t8.55\n",
      "4lck_C \t14.52\n",
      "4lck_E \t8.70\n",
      "4lck_F \t17.96\n",
      "4p9r_A \t5.17\n",
      "4qlm_A \t10.46\n",
      "4r4v_A \t11.41\n",
      "4xw7_A \t10.33\n",
      "5di4_A \t7.90\n",
      "5di4_B \t12.33\n",
      "5k7c_A \t8.83\n",
      "5k7c_B \t6.83\n",
      "5kpy_A \t5.10\n",
      "5lyu_A \t10.26\n",
      "5lyu_B \t10.09\n",
      "5nwq_A \t12.41\n",
      "5nwq_B \t12.28\n",
      "5swd_A \t7.10\n",
      "5swd_B \t6.97\n",
      "5swe_X \t6.00\n",
      "5tpy_A \t6.64\n",
      "5y85_A \t4.62\n",
      "5y85_B \t9.90\n",
      "5y85_C \t4.71\n",
      "5y85_D \t9.92\n",
      "6jq5_A \t15.83\n",
      "6jq5_B \t13.50\n",
      "6ol3_C \t17.35\n",
      "6p2h_A \t9.74\n",
      "6pmo_A \t21.02\n",
      "6pmo_B \t10.55\n",
      "6pom_A \t26.81\n",
      "6pom_B \t8.30\n",
      "6ufm_A \t9.95\n",
      "6ufm_B \t18.09\n",
      "########## 160000 10.59128689583909 4.7369726879034895\n"
     ]
    }
   ],
   "source": [
    "#1995000 rungen4\n",
    "for k in sorted(res.keys()):print(k, '\\t{:.2f}'.format(res[k]) )\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1y26_X \t8.55\n",
      "3owz_A \t10.06\n",
      "3owz_B \t11.89\n",
      "4l81_A \t6.49\n",
      "4lck_B \t12.50\n",
      "4lck_C \t15.58\n",
      "4lck_E \t12.51\n",
      "4lck_F \t21.96\n",
      "4p9r_A \t4.60\n",
      "4qlm_A \t21.80\n",
      "4r4v_A \t8.43\n",
      "4xw7_A \t9.87\n",
      "5di4_A \t11.01\n",
      "5di4_B \t18.61\n",
      "5k7c_A \t9.29\n",
      "5k7c_B \t9.14\n",
      "5kpy_A \t8.26\n",
      "5lyu_A \t13.45\n",
      "5lyu_B \t13.21\n",
      "5nwq_A \t11.01\n",
      "5nwq_B \t11.03\n",
      "5swd_A \t8.22\n",
      "5swd_B \t8.24\n",
      "5swe_X \t6.89\n",
      "5tpy_A \t8.96\n",
      "5y85_A \t8.98\n",
      "5y85_B \t8.99\n",
      "5y85_C \t9.19\n",
      "5y85_D \t9.02\n",
      "6jq5_A \t15.57\n",
      "6jq5_B \t13.22\n",
      "6ol3_C \t15.56\n",
      "6p2h_A \t9.82\n",
      "6pmo_A \t18.23\n",
      "6pmo_B \t12.34\n",
      "6pom_A \t36.01\n",
      "6pom_B \t10.81\n",
      "6ufm_A \t11.47\n",
      "6ufm_B \t17.29\n",
      "########## 160000 12.257543907849495 5.49879376009963\n"
     ]
    }
   ],
   "source": [
    "#1600000 noshape\n",
    "for k in sorted(res.keys()):print(k, '\\t{:.2f}'.format(res[k]) )\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1975000\n",
    "1y26_X \t5.87\n",
    "3owz_A \t8.10\n",
    "3owz_B \t13.02\n",
    "4l81_A \t6.00\n",
    "4lck_B \t8.50\n",
    "4lck_C \t14.70\n",
    "4lck_E \t8.65\n",
    "4lck_F \t18.09\n",
    "4p9r_A \t5.03\n",
    "4qlm_A \t10.42\n",
    "4r4v_A \t11.71\n",
    "4xw7_A \t10.27\n",
    "5di4_A \t7.77\n",
    "5di4_B \t12.31\n",
    "5k7c_A \t8.84\n",
    "5k7c_B \t6.83\n",
    "5kpy_A \t5.10\n",
    "5lyu_A \t10.30\n",
    "5lyu_B \t10.12\n",
    "5nwq_A \t12.54\n",
    "5nwq_B \t12.40\n",
    "5swd_A \t7.11\n",
    "5swd_B \t7.21\n",
    "5swe_X \t5.96\n",
    "5tpy_A \t6.58\n",
    "5y85_A \t4.67\n",
    "5y85_B \t10.02\n",
    "5y85_C \t4.78\n",
    "5y85_D \t10.03\n",
    "6jq5_A \t15.92\n",
    "6jq5_B \t13.58\n",
    "6ol3_C \t17.26\n",
    "6p2h_A \t9.67\n",
    "6pmo_A \t21.08\n",
    "6pmo_B \t10.54\n",
    "6pom_A \t26.92\n",
    "6pom_B \t8.25\n",
    "6ufm_A \t10.09\n",
    "6ufm_B \t18.00\n",
    "########## 160000 10.621520872363721 4.764119511208484\n",
    "\n",
    "\n",
    "2400000\n",
    "1y26_X \t7.90\n",
    "3owz_A \t9.46\n",
    "3owz_B \t13.15\n",
    "4l81_A \t11.72\n",
    "4lck_B \t23.63\n",
    "4lck_C \t20.44\n",
    "4lck_E \t24.04\n",
    "4lck_F \t23.96\n",
    "4p9r_A \t6.49\n",
    "4qlm_A \t17.07\n",
    "4r4v_A \t13.85\n",
    "4xw7_A \t12.03\n",
    "5di4_A \t9.92\n",
    "5di4_B \t11.21\n",
    "5k7c_A \t13.53\n",
    "5k7c_B \t9.31\n",
    "5kpy_A \t8.84\n",
    "5lyu_A \t10.15\n",
    "5lyu_B \t10.05\n",
    "5nwq_A \t14.97\n",
    "5nwq_B \t14.84\n",
    "5swd_A \t10.55\n",
    "5swd_B \t8.26\n",
    "5swe_X \t5.83\n",
    "5tpy_A \t13.29\n",
    "5y85_A \t7.53\n",
    "5y85_B \t10.73\n",
    "5y85_C \t7.59\n",
    "5y85_D \t10.79\n",
    "6jq5_A \t16.81\n",
    "6jq5_B \t15.97\n",
    "6ol3_C \t20.06\n",
    "6p2h_A \t13.81\n",
    "6pmo_A \t26.02\n",
    "6pmo_B \t13.16\n",
    "6pom_A \t25.06\n",
    "6pom_B \t10.88\n",
    "6ufm_A \t15.70\n",
    "6ufm_B \t27.87\n",
    "########## 160000 14.011461400666125 5.795682824852666\n",
    "\n",
    "2300000\n",
    "1y26_X \t7.48\n",
    "3owz_A \t9.70\n",
    "3owz_B \t12.10\n",
    "4l81_A \t8.74\n",
    "4lck_B \t18.13\n",
    "4lck_C \t18.71\n",
    "4lck_E \t18.36\n",
    "4lck_F \t20.75\n",
    "4p9r_A \t5.79\n",
    "4qlm_A \t16.26\n",
    "4r4v_A \t11.71\n",
    "4xw7_A \t11.97\n",
    "5di4_A \t9.95\n",
    "5di4_B \t12.93\n",
    "5k7c_A \t11.75\n",
    "5k7c_B \t9.43\n",
    "5kpy_A \t7.27\n",
    "5lyu_A \t9.06\n",
    "5lyu_B \t8.97\n",
    "5nwq_A \t15.69\n",
    "5nwq_B \t15.73\n",
    "5swd_A \t9.36\n",
    "5swd_B \t7.68\n",
    "5swe_X \t6.27\n",
    "5tpy_A \t10.21\n",
    "5y85_A \t8.40\n",
    "5y85_B \t10.83\n",
    "5y85_C \t8.48\n",
    "5y85_D \t10.81\n",
    "6jq5_A \t15.38\n",
    "6jq5_B \t15.38\n",
    "6ol3_C \t17.66\n",
    "6p2h_A \t10.46\n",
    "6pmo_A \t23.45\n",
    "6pmo_B \t10.85\n",
    "6pom_A \t26.50\n",
    "6pom_B \t10.26\n",
    "6ufm_A \t15.07\n",
    "6ufm_B \t24.85\n",
    "########## 160000 12.88154681998209 5.086736964623545\n",
    "\n",
    "2200000\n",
    "1y26_X \t6.01\n",
    "3owz_A \t8.14\n",
    "3owz_B \t10.25\n",
    "4l81_A \t6.35\n",
    "4lck_B \t12.61\n",
    "4lck_C \t15.43\n",
    "4lck_E \t12.96\n",
    "4lck_F \t18.10\n",
    "4p9r_A \t5.00\n",
    "4qlm_A \t12.26\n",
    "4r4v_A \t12.55\n",
    "4xw7_A \t11.02\n",
    "5di4_A \t9.76\n",
    "5di4_B \t13.93\n",
    "5k7c_A \t11.40\n",
    "5k7c_B \t9.61\n",
    "5kpy_A \t5.11\n",
    "5lyu_A \t9.68\n",
    "5lyu_B \t9.55\n",
    "5nwq_A \t12.95\n",
    "5nwq_B \t12.97\n",
    "5swd_A \t8.19\n",
    "5swd_B \t7.02\n",
    "5swe_X \t5.99\n",
    "5tpy_A \t7.60\n",
    "5y85_A \t7.68\n",
    "5y85_B \t10.60\n",
    "5y85_C \t7.80\n",
    "5y85_D \t10.55\n",
    "6jq5_A \t15.72\n",
    "6jq5_B \t15.81\n",
    "6ol3_C \t17.70\n",
    "6p2h_A \t11.53\n",
    "6pmo_A \t17.29\n",
    "6pmo_B \t10.06\n",
    "6pom_A \t28.46\n",
    "6pom_B \t8.54\n",
    "6ufm_A \t12.27\n",
    "6ufm_B \t19.23\n",
    "########## 160000 11.479109175466833 4.57017314098255\n",
    "\n",
    "2100000\n",
    "1y26_X \t7.47\n",
    "3owz_A \t7.44\n",
    "3owz_B \t10.20\n",
    "4l81_A \t6.93\n",
    "4lck_B \t11.11\n",
    "4lck_C \t17.18\n",
    "4lck_E \t11.26\n",
    "4lck_F \t20.87\n",
    "4p9r_A \t4.87\n",
    "4qlm_A \t14.33\n",
    "4r4v_A \t11.28\n",
    "4xw7_A \t10.05\n",
    "5di4_A \t9.10\n",
    "5di4_B \t13.37\n",
    "5k7c_A \t10.36\n",
    "5k7c_B \t8.86\n",
    "5kpy_A \t6.15\n",
    "5lyu_A \t9.54\n",
    "5lyu_B \t9.35\n",
    "5nwq_A \t13.67\n",
    "5nwq_B \t13.69\n",
    "5swd_A \t8.36\n",
    "5swd_B \t8.23\n",
    "5swe_X \t5.43\n",
    "5tpy_A \t8.07\n",
    "5y85_A \t4.93\n",
    "5y85_B \t10.45\n",
    "5y85_C \t5.06\n",
    "5y85_D \t10.39\n",
    "6jq5_A \t14.33\n",
    "6jq5_B \t14.36\n",
    "6ol3_C \t18.00\n",
    "6p2h_A \t10.94\n",
    "6pmo_A \t18.43\n",
    "6pmo_B \t10.27\n",
    "6pom_A \t27.68\n",
    "6pom_B \t7.91\n",
    "6ufm_A \t9.54\n",
    "6ufm_B \t21.06\n",
    "########## 160000 11.29468299630205 4.874101806539017\n",
    "\n",
    "\n",
    "2000000\n",
    "1y26_X \t7.73\n",
    "3owz_A \t11.06\n",
    "3owz_B \t11.64\n",
    "4l81_A \t9.80\n",
    "4lck_B \t11.56\n",
    "4lck_C \t20.84\n",
    "4lck_E \t11.64\n",
    "4lck_F \t22.76\n",
    "4p9r_A \t5.85\n",
    "4qlm_A \t12.83\n",
    "4r4v_A \t10.47\n",
    "4xw7_A \t10.48\n",
    "5di4_A \t10.75\n",
    "5di4_B \t12.24\n",
    "5k7c_A \t9.75\n",
    "5k7c_B \t7.14\n",
    "5kpy_A \t6.01\n",
    "5lyu_A \t8.76\n",
    "5lyu_B \t8.63\n",
    "5nwq_A \t14.28\n",
    "5nwq_B \t14.11\n",
    "5swd_A \t9.84\n",
    "5swd_B \t9.54\n",
    "5swe_X \t5.81\n",
    "5tpy_A \t8.71\n",
    "5y85_A \t5.37\n",
    "5y85_B \t12.16\n",
    "5y85_C \t5.44\n",
    "5y85_D \t12.18\n",
    "6jq5_A \t17.67\n",
    "6jq5_B \t15.69\n",
    "6ol3_C \t19.65\n",
    "6p2h_A \t12.49\n",
    "6pmo_A \t26.11\n",
    "6pmo_B \t10.24\n",
    "6pom_A \t25.19\n",
    "6pom_B \t8.87\n",
    "6ufm_A \t17.84\n",
    "6ufm_B \t25.78\n",
    "########## 160000 12.485229141384623 5.572772716620793\n",
    "\n",
    "1900000\n",
    "1y26_X \t6.06\n",
    "3owz_A \t8.27\n",
    "3owz_B \t13.14\n",
    "4l81_A \t6.08\n",
    "4lck_B \t8.69\n",
    "4lck_C \t14.71\n",
    "4lck_E \t8.84\n",
    "4lck_F \t17.80\n",
    "4p9r_A \t5.11\n",
    "4qlm_A \t10.68\n",
    "4r4v_A \t11.26\n",
    "4xw7_A \t10.07\n",
    "5di4_A \t7.80\n",
    "5di4_B \t12.33\n",
    "5k7c_A \t8.82\n",
    "5k7c_B \t6.83\n",
    "5kpy_A \t5.14\n",
    "5lyu_A \t10.18\n",
    "5lyu_B \t9.99\n",
    "5nwq_A \t12.77\n",
    "5nwq_B \t12.63\n",
    "5swd_A \t7.06\n",
    "5swd_B \t7.08\n",
    "5swe_X \t5.85\n",
    "5tpy_A \t6.64\n",
    "5y85_A \t4.62\n",
    "5y85_B \t9.87\n",
    "5y85_C \t4.71\n",
    "5y85_D \t9.89\n",
    "6jq5_A \t15.88\n",
    "6jq5_B \t13.51\n",
    "6ol3_C \t17.56\n",
    "6p2h_A \t9.80\n",
    "6pmo_A \t21.19\n",
    "6pmo_B \t10.69\n",
    "6pom_A \t26.67\n",
    "6pom_B \t8.30\n",
    "6ufm_A \t10.14\n",
    "6ufm_B \t18.23\n",
    "########## 160000 10.63867157782212 4.752127913709705\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1y26_X \t8.42\n",
      "3owz_A \t11.28\n",
      "3owz_B \t13.08\n",
      "4l81_A \t13.69\n",
      "4lck_B \t23.59\n",
      "4lck_C \t28.73\n",
      "4lck_E \t24.01\n",
      "4lck_F \t29.49\n",
      "4p9r_A \t8.32\n",
      "4qlm_A \t17.49\n",
      "4r4v_A \t14.99\n",
      "4xw7_A \t10.79\n",
      "5di4_A \t9.64\n",
      "5di4_B \t11.56\n",
      "5k7c_A \t12.84\n",
      "5k7c_B \t8.68\n",
      "5kpy_A \t11.44\n",
      "5lyu_A \t9.90\n",
      "5lyu_B \t9.79\n",
      "5nwq_A \t15.06\n",
      "5nwq_B \t14.94\n",
      "5swd_A \t11.84\n",
      "5swd_B \t10.79\n",
      "5swe_X \t8.05\n",
      "5tpy_A \t12.79\n",
      "5y85_A \t7.30\n",
      "5y85_B \t10.60\n",
      "5y85_C \t7.36\n",
      "5y85_D \t10.63\n",
      "6jq5_A \t20.92\n",
      "6jq5_B \t18.88\n",
      "6ol3_C \t25.46\n",
      "6p2h_A \t13.91\n",
      "6pmo_A \t23.02\n",
      "6pmo_B \t15.45\n",
      "6pom_A \t26.89\n",
      "6pom_B \t12.42\n",
      "6ufm_A \t21.86\n",
      "6ufm_B \t26.21\n",
      "########## 160000 15.18204196380455 6.43864419113794\n"
     ]
    }
   ],
   "source": [
    "#1955000 rungen4\n",
    "for k in sorted(res.keys()):print(k, '\\t{:.2f}'.format(res[k]) )\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 11.375092284751284 4.846355918498879\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'4l81_A': 6.865182801814916,\n",
       " '5di4_A': 11.555158477264342,\n",
       " '5nwq_A': 14.095523796748436,\n",
       " '6ufm_A': 12.530832648494973,\n",
       " '5swd_B': 8.154806425077863,\n",
       " '6jq5_B': 15.259354607325594,\n",
       " '3owz_B': 12.336093541193272,\n",
       " '5y85_C': 5.483982784470177,\n",
       " '5nwq_B': 14.084759892250593,\n",
       " '5k7c_A': 9.536208107035081,\n",
       " '6p2h_A': 10.186704017944688,\n",
       " '4lck_C': 16.189527808707734,\n",
       " '5y85_B': 11.296463141661947,\n",
       " '5y85_D': 11.301091744900823,\n",
       " '4r4v_A': 10.805838569924191,\n",
       " '4p9r_A': 5.405008364494112,\n",
       " '5tpy_A': 6.74604618000895,\n",
       " '5swe_X': 5.756935358368716,\n",
       " '6pmo_A': 20.74025678234635,\n",
       " '5lyu_A': 9.782494083586855,\n",
       " '3owz_A': 8.404117042364254,\n",
       " '6pom_B': 9.64386124456689,\n",
       " '5lyu_B': 9.606434590893487,\n",
       " '5y85_A': 5.404054978995964,\n",
       " '1y26_X': 7.320779688322033,\n",
       " '6ufm_B': 20.016082895988642,\n",
       " '6pom_A': 27.44579512349206,\n",
       " '6ol3_C': 19.92716618600957,\n",
       " '5k7c_B': 6.7761105067717216,\n",
       " '6pmo_B': 11.735737301838768,\n",
       " '4xw7_A': 10.088133362249957,\n",
       " '6jq5_A': 15.983817452533502,\n",
       " '4lck_F': 17.213369078053912,\n",
       " '4lck_E': 9.155427893319255,\n",
       " '5swd_A': 7.891118450984894,\n",
       " '4qlm_A': 11.558700154308593,\n",
       " '4lck_B': 9.051212832035693,\n",
       " '5di4_B': 12.853563540752372,\n",
       " '5kpy_A': 5.440847648198949}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#310000 noshape lr4\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 14.949084288463359 5.157974955091217\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'3owz_B': 17.37867384015171,\n",
       " '5lyu_B': 13.230658794551395,\n",
       " '5k7c_A': 18.386788801297133,\n",
       " '4l81_A': 10.78565950321137,\n",
       " '5kpy_A': 8.743476928688606,\n",
       " '5di4_A': 22.42769367364306,\n",
       " '5y85_B': 15.700335604321424,\n",
       " '6pmo_B': 12.739782622410537,\n",
       " '6pmo_A': 18.40093622077801,\n",
       " '5swd_B': 9.375269011730197,\n",
       " '4qlm_A': 14.238766210490436,\n",
       " '5nwq_B': 18.417013513103587,\n",
       " '6pom_B': 14.065960996804815,\n",
       " '6ol3_C': 22.79232144806704,\n",
       " '4xw7_A': 15.49146937930254,\n",
       " '5y85_A': 8.536171661367534,\n",
       " '4r4v_A': 17.248123420215016,\n",
       " '6ufm_B': 16.724524121134387,\n",
       " '6jq5_B': 13.749213561601644,\n",
       " '4lck_B': 13.409416019527546,\n",
       " '6ufm_A': 14.666181714800784,\n",
       " '5lyu_A': 13.385130056996692,\n",
       " '6jq5_A': 20.487975898898203,\n",
       " '5k7c_B': 6.721226999101195,\n",
       " '5tpy_A': 13.835887637867916,\n",
       " '5nwq_A': 18.394130176559408,\n",
       " '5swd_A': 8.392015321887493,\n",
       " '5y85_D': 15.685969893261118,\n",
       " '5di4_B': 15.07246905638972,\n",
       " '5y85_C': 8.554174790639554,\n",
       " '4lck_E': 13.43314635267177,\n",
       " '4lck_F': 23.968457944518462,\n",
       " '4lck_C': 20.09354865104391,\n",
       " '3owz_A': 17.973200475599953,\n",
       " '1y26_X': 8.31697021749849,\n",
       " '6pom_A': 31.4024642677459,\n",
       " '4p9r_A': 8.242429807011083,\n",
       " '5swe_X': 7.82394395338106,\n",
       " '6p2h_A': 14.722708701800265}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#870000 noshape lr3\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 11.7003980813843 5.296577915963619\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'5di4_A': 12.388673053378138,\n",
       " '5y85_D': 10.784572834507708,\n",
       " '5y85_A': 5.1726576175091274,\n",
       " '5lyu_B': 10.288918135283215,\n",
       " '6jq5_B': 16.191754216905803,\n",
       " '5nwq_B': 16.55389930270833,\n",
       " '6ufm_A': 9.44963797719801,\n",
       " '3owz_B': 10.82614553025169,\n",
       " '4r4v_A': 11.502206102181619,\n",
       " '5lyu_A': 10.23522813663797,\n",
       " '5y85_C': 5.264079352131667,\n",
       " '5kpy_A': 5.35731060964928,\n",
       " '1y26_X': 8.597518126224703,\n",
       " '5di4_B': 10.55595890409277,\n",
       " '4xw7_A': 10.589228612656393,\n",
       " '5tpy_A': 7.751297986343173,\n",
       " '5nwq_A': 16.648605562761194,\n",
       " '5swe_X': 5.687336145864809,\n",
       " '4lck_C': 21.305946838822774,\n",
       " '6ufm_B': 19.40537438842397,\n",
       " '3owz_A': 10.80414973531385,\n",
       " '6pmo_B': 11.192813458798893,\n",
       " '4lck_E': 9.487672365979533,\n",
       " '4lck_F': 21.649053192873495,\n",
       " '5y85_B': 10.799188854167422,\n",
       " '4qlm_A': 11.75857872664283,\n",
       " '4p9r_A': 4.495704804292752,\n",
       " '5swd_A': 8.178779896253756,\n",
       " '5k7c_B': 7.007818702314676,\n",
       " '4l81_A': 6.113149699768062,\n",
       " '6jq5_A': 17.470006579195985,\n",
       " '5swd_B': 7.3639591061121825,\n",
       " '6ol3_C': 19.14147530942568,\n",
       " '6pom_B': 9.207475705080617,\n",
       " '4lck_B': 9.277967674668814,\n",
       " '6pom_A': 26.91843745797931,\n",
       " '6p2h_A': 12.181938574411346,\n",
       " '6pmo_A': 20.911811854292008,\n",
       " '5k7c_A': 7.799194042884179}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#160000 rungen4 masked\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 11.390999627689638 5.16577255607938\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'6p2h_A': 11.269387086115078,\n",
       " '5y85_D': 8.991249488664861,\n",
       " '6pmo_A': 24.224508982192802,\n",
       " '5di4_B': 9.96344767682451,\n",
       " '5y85_C': 6.800646510696244,\n",
       " '4lck_C': 15.387565609737392,\n",
       " '5y85_B': 9.018464438988355,\n",
       " '5swd_A': 8.100016124390557,\n",
       " '6jq5_A': 17.561522054586153,\n",
       " '4r4v_A': 9.355646820856926,\n",
       " '6ol3_C': 19.79033065821403,\n",
       " '5kpy_A': 5.0354045273729335,\n",
       " '4qlm_A': 16.058186976251708,\n",
       " '6pom_B': 9.491285959428435,\n",
       " '4xw7_A': 10.891065697767049,\n",
       " '5tpy_A': 7.468106445046586,\n",
       " '6pom_A': 25.653407522024107,\n",
       " '5nwq_A': 16.072691934936586,\n",
       " '5nwq_B': 16.04409177916679,\n",
       " '6ufm_B': 17.475742136566826,\n",
       " '5k7c_A': 8.366168133279652,\n",
       " '5di4_A': 9.233201260555573,\n",
       " '5lyu_A': 9.44454380192437,\n",
       " '6pmo_B': 13.522595078861144,\n",
       " '4lck_E': 8.850932348142763,\n",
       " '5k7c_B': 6.843327263909725,\n",
       " '5swe_X': 5.748460184249395,\n",
       " '4p9r_A': 4.9995585957601705,\n",
       " '5y85_A': 6.755095068625892,\n",
       " '3owz_B': 11.396424707942645,\n",
       " '3owz_A': 9.008553445959233,\n",
       " '4lck_B': 8.759838801852839,\n",
       " '5swd_B': 7.633005610864136,\n",
       " '4lck_F': 20.064668065159193,\n",
       " '4l81_A': 4.882589897472347,\n",
       " '5lyu_B': 9.50409299304583,\n",
       " '6ufm_A': 9.055315752561647,\n",
       " '6jq5_B': 17.004055969976736,\n",
       " '1y26_X': 8.523790069924717}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#160000 rungen4 masked\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 7.666204124821697 3.0492147752603165\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'4lck_F': 8.596702970613386,\n",
       " '6ufm_B': 7.055895897355952,\n",
       " '5swd_B': 5.969215303100391,\n",
       " '4qlm_A': 6.8346957749201485,\n",
       " '5swe_X': 4.598959197468485,\n",
       " '6pmo_B': 7.820589874689821,\n",
       " '6pom_B': 7.804921970110276,\n",
       " '6jq5_B': 5.216373698033285,\n",
       " '5di4_B': 9.03692764303525,\n",
       " '5k7c_B': 7.476494196321595,\n",
       " '4p9r_A': 3.9144981445855676,\n",
       " '4lck_B': 7.141360074143546,\n",
       " '6p2h_A': 6.143843891505795,\n",
       " '5lyu_B': 11.161341346514526,\n",
       " '1y26_X': 4.920655810401087,\n",
       " '5nwq_B': 4.480416202167558,\n",
       " '4l81_A': 4.522990017263543,\n",
       " '5y85_D': 5.056462668667498,\n",
       " '6pmo_A': 15.156656840851285,\n",
       " '4lck_C': 8.694441983186438,\n",
       " '4lck_E': 9.472210387819361,\n",
       " '5tpy_A': 5.182812676066514,\n",
       " '5y85_A': 5.985986986264634,\n",
       " '5lyu_A': 10.095118190038272,\n",
       " '4r4v_A': 8.012835587871946,\n",
       " '3owz_A': 12.045915949761664,\n",
       " '6jq5_A': 4.288465328401641,\n",
       " '5k7c_A': 4.782239224579082,\n",
       " '5swd_A': 7.745922111664744,\n",
       " '3owz_B': 6.899203738154669,\n",
       " '5kpy_A': 5.570819556117491,\n",
       " '5nwq_A': 8.102418346556139,\n",
       " '6ol3_C': 12.404699198882103,\n",
       " '5y85_B': 4.935316853117452,\n",
       " '5y85_C': 6.9519082565382275,\n",
       " '4xw7_A': 7.935010108825828,\n",
       " '6ufm_A': 5.941321537829513,\n",
       " '6pom_A': 14.794266159508902,\n",
       " '5di4_A': 16.23204716511258}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#160000 att50 masked\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 14.367840365059092 5.034719921591547\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'6ufm_A': 12.547738479476871,\n",
       " '5tpy_A': 10.818123884125507,\n",
       " '6pmo_A': 19.831047467848844,\n",
       " '6pom_B': 9.320912459961834,\n",
       " '3owz_A': 13.627075033971126,\n",
       " '6jq5_B': 20.209630317470452,\n",
       " '5y85_A': 11.474242746005203,\n",
       " '5kpy_A': 12.473678917689597,\n",
       " '4l81_A': 15.272049012327946,\n",
       " '4r4v_A': 12.57121251974092,\n",
       " '1y26_X': 13.518971576405956,\n",
       " '5swd_B': 8.97324260949576,\n",
       " '4lck_E': 11.80874052622854,\n",
       " '4lck_F': 23.810495983796713,\n",
       " '4xw7_A': 13.266554873701619,\n",
       " '5lyu_B': 11.788644265793975,\n",
       " '5di4_A': 13.040274573536427,\n",
       " '5y85_D': 8.253659467190573,\n",
       " '6pmo_B': 12.340324737000403,\n",
       " '6p2h_A': 14.826786245762664,\n",
       " '5k7c_B': 8.61464483186912,\n",
       " '5swe_X': 9.415037135554057,\n",
       " '4p9r_A': 6.777705182627851,\n",
       " '5k7c_A': 11.929539309637292,\n",
       " '5y85_B': 8.149829806470327,\n",
       " '4lck_B': 11.578102957890884,\n",
       " '4qlm_A': 16.184813737139415,\n",
       " '6ol3_C': 20.354786542692313,\n",
       " '4lck_C': 25.069957976514296,\n",
       " '5y85_C': 11.684018426605117,\n",
       " '6pom_A': 28.608333906021834,\n",
       " '5swd_A': 9.952957540018692,\n",
       " '5di4_B': 18.02617613686437,\n",
       " '3owz_B': 14.24822802768542,\n",
       " '5nwq_B': 17.60534823423164,\n",
       " '5lyu_A': 11.917392232422575,\n",
       " '6jq5_A': 21.279757266827914,\n",
       " '5nwq_A': 17.736568610269973,\n",
       " '6ufm_B': 21.43917067843052}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100000 att50\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6p2h_A 11.162280306081845\n",
      "6pom_A 25.528625577410786\n",
      "4r4v_A 12.085027174370959\n",
      "3owz_B 12.153343693272584\n",
      "5k7c_A 9.60869848967086\n",
      "3owz_A 10.477141128747956\n",
      "4xw7_A 8.139719233834818\n",
      "5y85_B 9.722929716133184\n",
      "5y85_C 7.7950785290903735\n",
      "5kpy_A 6.767988562958947\n",
      "5y85_A 7.802483849009158\n",
      "6pom_B 9.484222969261541\n",
      "6ol3_C 20.529720711732146\n",
      "5lyu_A 8.755519997825482\n",
      "5lyu_B 8.72732098984014\n",
      "4p9r_A 6.025912011348834\n",
      "5k7c_B 6.895466613349384\n",
      "5y85_D 9.738641620355985\n",
      "5di4_A 10.29230981082295\n",
      "5di4_B 10.16451439138719\n",
      "4qlm_A 16.09903687517746\n",
      "5tpy_A 7.778533008565817\n",
      "6jq5_A 18.101763961016193\n",
      "6jq5_B 18.10235283330317\n",
      "4l81_A 5.183908075591263\n",
      "### 11.08490160520636 4.827584534472993\n"
     ]
    }
   ],
   "source": [
    "shown_puzzles = ['4r4v', '4p9r', '6jq5', '6pom',# '6ufm',\n",
    "                 #'6pmo', \n",
    "                 '6ol3', \n",
    "                 '4qlm' ,'5y85', '5di4', '4xw7', '5tpy', '5k7c', '3owz', \n",
    "                 '4l81', #'4lck', \n",
    "                 #'5nwq', \n",
    "                 '5kpy', '5lyu', '6p2h']\n",
    "l = []\n",
    "for k in res.keys():\n",
    "    \n",
    "    if k[:4] in shown_puzzles:\n",
    "        l.append(res[k])\n",
    "        print(k, res[k])\n",
    "print('###', np.mean(l), np.std(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.413690942271653"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 11.941556360329182 5.218508662486335\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'4r4v_A': 11.655276232809998,\n",
       " '3owz_A': 9.819894327296131,\n",
       " '5k7c_A': 9.868117354094762,\n",
       " '6jq5_B': 15.8820495610741,\n",
       " '4lck_E': 10.635805343537807,\n",
       " '4p9r_A': 5.375504909059298,\n",
       " '5y85_B': 10.475907219721199,\n",
       " '1y26_X': 6.108797704981592,\n",
       " '6ufm_A': 12.58050943709695,\n",
       " '5kpy_A': 6.330532329547898,\n",
       " '4lck_F': 24.94010854693851,\n",
       " '5y85_D': 10.428970794038838,\n",
       " '5nwq_B': 16.400925440065357,\n",
       " '6pom_A': 25.17043780011663,\n",
       " '5swe_X': 7.256371968890405,\n",
       " '6jq5_A': 16.19862453053055,\n",
       " '5y85_A': 6.928278502741838,\n",
       " '5swd_B': 9.0429637508126,\n",
       " '4lck_C': 17.347391335203426,\n",
       " '5lyu_B': 9.26160230804705,\n",
       " '4qlm_A': 15.270353978481126,\n",
       " '5k7c_B': 6.762871461248681,\n",
       " '5swd_A': 8.833381648645629,\n",
       " '5di4_B': 12.038752400873793,\n",
       " '6pmo_B': 11.699134919578889,\n",
       " '4xw7_A': 10.608999868636683,\n",
       " '5y85_C': 6.98965734196087,\n",
       " '6ol3_C': 20.3963341526994,\n",
       " '6ufm_B': 17.32062333222148,\n",
       " '5lyu_A': 9.366994008271275,\n",
       " '6p2h_A': 10.296270319081879,\n",
       " '5di4_A': 9.91612173328536,\n",
       " '4lck_B': 10.562884138299959,\n",
       " '6pom_B': 8.939893717012254,\n",
       " '5nwq_A': 16.45078720416342,\n",
       " '3owz_B': 10.728036090785189,\n",
       " '6pmo_A': 24.871125498459932,\n",
       " '5tpy_A': 7.290823730148948,\n",
       " '4l81_A': 5.669583112378411}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#230000 rungen4 with null mask diags\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 13.944135413238133 5.047425041814914\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'5nwq_B': 18.799771488716566,\n",
       " '5k7c_A': 10.664718040586452,\n",
       " '5nwq_A': 18.899523168111305,\n",
       " '6jq5_B': 18.972148510715694,\n",
       " '5tpy_A': 13.63103290339181,\n",
       " '5di4_B': 17.88641391234914,\n",
       " '5lyu_A': 11.613208333463161,\n",
       " '5y85_A': 6.955586829078792,\n",
       " '3owz_A': 11.619343784712678,\n",
       " '6pom_A': 26.502237542276976,\n",
       " '6pmo_B': 12.529696991169763,\n",
       " '4lck_F': 15.093629316244142,\n",
       " '5y85_D': 11.929112679772901,\n",
       " '4qlm_A': 14.418675155127902,\n",
       " '5lyu_B': 11.652424264455979,\n",
       " '6ufm_A': 16.54403464453679,\n",
       " '5swd_A': 7.190932347075438,\n",
       " '6pom_B': 10.918828537276982,\n",
       " '6ufm_B': 24.160862288993215,\n",
       " '5swe_X': 9.61324792201622,\n",
       " '4l81_A': 7.73814656902417,\n",
       " '4xw7_A': 17.215764169431846,\n",
       " '6pmo_A': 17.24098486226244,\n",
       " '4r4v_A': 14.037952406877384,\n",
       " '5y85_C': 7.113056785731472,\n",
       " '6p2h_A': 13.504553189600395,\n",
       " '1y26_X': 8.531686268758994,\n",
       " '4lck_B': 19.463043055894556,\n",
       " '5k7c_B': 8.280843938868959,\n",
       " '4lck_C': 17.9206263545709,\n",
       " '5kpy_A': 11.317361674107017,\n",
       " '5swd_B': 8.789793239917287,\n",
       " '4p9r_A': 5.481892675933262,\n",
       " '5di4_A': 10.53506610596987,\n",
       " '3owz_B': 13.555189565476073,\n",
       " '4lck_E': 19.782754356739673,\n",
       " '6jq5_A': 18.387824010346108,\n",
       " '5y85_B': 11.92378174725625,\n",
       " '6ol3_C': 23.40553147944866}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#120000 rungen4 with null mask diags\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 13.93559509296611 5.082216676373168\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'6p2h_A': 13.47564888785534,\n",
       " '6ol3_C': 23.485001179384454,\n",
       " '5kpy_A': 11.348170329143967,\n",
       " '4lck_C': 17.96637881651529,\n",
       " '6pom_B': 10.915038739882592,\n",
       " '6jq5_B': 19.049956119609842,\n",
       " '5y85_D': 11.914167728699523,\n",
       " '1y26_X': 8.598647860470129,\n",
       " '6jq5_A': 18.428626368819227,\n",
       " '5swd_A': 7.090107791434572,\n",
       " '5lyu_A': 11.619618790817524,\n",
       " '4l81_A': 7.712101658388376,\n",
       " '6ufm_B': 24.123579660947787,\n",
       " '3owz_B': 13.518474059843795,\n",
       " '5swd_B': 8.751038175480419,\n",
       " '6pmo_A': 17.280945479192415,\n",
       " '5di4_B': 18.06733380415101,\n",
       " '4xw7_A': 17.188996712297644,\n",
       " '5k7c_A': 10.549826790569977,\n",
       " '5nwq_A': 18.906933287677237,\n",
       " '5lyu_B': 11.655286271974532,\n",
       " '4r4v_A': 14.048222112654102,\n",
       " '5y85_A': 7.004203711769056,\n",
       " '6pom_A': 26.54792275352379,\n",
       " '4p9r_A': 5.427910086825125,\n",
       " '4qlm_A': 14.509821342021374,\n",
       " '4lck_E': 19.76631446704196,\n",
       " '4lck_F': 15.097374423962655,\n",
       " '5k7c_B': 7.718946369988662,\n",
       " '5tpy_A': 13.61519112182288,\n",
       " '3owz_A': 11.686286560875345,\n",
       " '6ufm_A': 16.552428805083167,\n",
       " '4lck_B': 19.444751002480007,\n",
       " '5swe_X': 9.452665343930592,\n",
       " '5y85_C': 7.164586145364298,\n",
       " '5y85_B': 11.909654914544703,\n",
       " '6pmo_B': 12.541067161090242,\n",
       " '5di4_A': 10.576814320085825,\n",
       " '5nwq_B': 18.778169469458827}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#120000 rungen4\n",
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 12.78105336949454 4.207511695648259\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'6pmo_A': 17.118501844586373,\n",
       " '5y85_C': 9.621299418869478,\n",
       " '4qlm_A': 14.046460788830224,\n",
       " '5nwq_A': 16.757827237942955,\n",
       " '5tpy_A': 12.336262454522418,\n",
       " '4r4v_A': 10.205552032180917,\n",
       " '5y85_D': 10.939521502360687,\n",
       " '5swd_B': 9.331508537156392,\n",
       " '4p9r_A': 5.602169136142471,\n",
       " '5swe_X': 8.850249695006783,\n",
       " '5k7c_A': 11.020122172088612,\n",
       " '6p2h_A': 12.922492258391468,\n",
       " '1y26_X': 8.492427958483606,\n",
       " '6ufm_B': 23.09119212316719,\n",
       " '4lck_B': 14.235353079669336,\n",
       " '3owz_A': 10.583184894178139,\n",
       " '5di4_A': 12.278357598474983,\n",
       " '5nwq_B': 16.588587230009324,\n",
       " '5lyu_A': 11.496128761632443,\n",
       " '4lck_F': 21.201378872047815,\n",
       " '6pom_B': 10.108319578665682,\n",
       " '6ufm_A': 12.981423177567223,\n",
       " '4xw7_A': 15.436635088193722,\n",
       " '5y85_B': 10.969312002359196,\n",
       " '5di4_B': 18.962410600213413,\n",
       " '6ol3_C': 19.68255435331192,\n",
       " '5swd_A': 8.392233774977129,\n",
       " '5kpy_A': 8.887638295582791,\n",
       " '4l81_A': 8.511443248729003}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 12.722257769400764 4.200617323752026\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'5y85_A': 8.162658680374559,\n",
       " '5tpy_A': 13.581430060246632,\n",
       " '6jq5_B': 17.91444273329207,\n",
       " '4lck_C': 16.718266159018686,\n",
       " '5swe_X': 6.399720300877206,\n",
       " '6p2h_A': 11.06507848366936,\n",
       " '5k7c_A': 12.215076007905743,\n",
       " '5y85_B': 9.662105426251722,\n",
       " '5lyu_A': 11.435556177968795,\n",
       " '6pmo_A': 18.108203922747187,\n",
       " '4lck_F': 17.400794700866882,\n",
       " '5nwq_A': 17.504133576885767,\n",
       " '5y85_C': 8.260480619651252,\n",
       " '6ufm_B': 20.14115673597179,\n",
       " '3owz_A': 10.464380436155002,\n",
       " '4lck_B': 14.916529020860144,\n",
       " '5k7c_B': 8.6056062420164,\n",
       " '5swd_B': 7.721676150860696,\n",
       " '6ufm_A': 16.414178127192066,\n",
       " '4lck_E': 15.174924043988444,\n",
       " '4r4v_A': 10.779501602799002,\n",
       " '5di4_A': 10.715068111262156,\n",
       " '5di4_B': 17.75754821988972,\n",
       " '6jq5_A': 18.279825160330343,\n",
       " '5swd_A': 7.442270909105919,\n",
       " '4l81_A': 9.017466186620466,\n",
       " '4p9r_A': 5.8905866048683135,\n",
       " '4qlm_A': 14.474553141545046}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 6.779386511209554 1.3530090918963067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3owz_B': 8.174813433772364,\n",
       " '4lck_E': 7.258030091716915,\n",
       " '5y85_C': 6.959630318775557,\n",
       " '6jq5_A': 5.133619477562675,\n",
       " '5di4_B': 8.36584037054996,\n",
       " '5nwq_B': 4.60578503303183,\n",
       " '5kpy_A': 5.5416394850232695,\n",
       " '5y85_B': 5.877470503824932,\n",
       " '4r4v_A': 9.556602352827788,\n",
       " '3owz_A': 6.559220041639288,\n",
       " '4lck_C': 6.836326949835963,\n",
       " '4xw7_A': 6.976583195176355,\n",
       " '4lck_F': 9.139404773149908,\n",
       " '5nwq_A': 8.738278162704315,\n",
       " '6jq5_B': 5.486782702092725,\n",
       " '5di4_A': 4.447543348362462,\n",
       " '6p2h_A': 5.67788149452197,\n",
       " '5tpy_A': 6.095741550758288,\n",
       " '4lck_B': 6.967497814568614,\n",
       " '6pmo_A': 6.769170134519883,\n",
       " '4qlm_A': 6.911077887114852,\n",
       " '5k7c_A': 6.460753758108421,\n",
       " '4p9r_A': 4.81422366871124,\n",
       " '6ufm_A': 7.8092851141896755,\n",
       " '6pmo_B': 8.690013800378845,\n",
       " '5swd_A': 6.848194642907476,\n",
       " '5swd_B': 6.342025696832388}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 160000 7.529652119241821 2.614057022786943\n"
     ]
    }
   ],
   "source": [
    "print('##########', n_iter, np.mean(list(res.values())), np.std(list(res.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4lck_F': 10.552308250701083,\n",
       " '6pom_B': 6.660278124326192,\n",
       " '5di4_B': 7.84693246065069,\n",
       " '5tpy_A': 5.751722433153528,\n",
       " '6jq5_A': 7.744868005288602,\n",
       " '6p2h_A': 5.894041788180107,\n",
       " '5di4_A': 4.369317059208708,\n",
       " '4r4v_A': 7.662352036233554,\n",
       " '6pmo_B': 8.850689995466935,\n",
       " '4p9r_A': 4.190104069620308,\n",
       " '6jq5_B': 5.132604388039205,\n",
       " '6pom_A': 13.146860709025939,\n",
       " '5lyu_A': 9.134855547994437,\n",
       " '5y85_D': 10.075328722730633,\n",
       " '3owz_B': 6.20346812374688,\n",
       " '5k7c_A': 5.504099754435188,\n",
       " '5swd_A': 6.666351762942676,\n",
       " '6ol3_C': 11.422838018643647,\n",
       " '5swe_X': 4.885611033239375,\n",
       " '6ufm_B': 4.924021702845575,\n",
       " '5lyu_B': 10.269051040766845,\n",
       " '4xw7_A': 7.8589510405454375,\n",
       " '6ufm_A': 5.7646805842937905,\n",
       " '5kpy_A': 5.128534022348128,\n",
       " '3owz_A': 4.929328764030417,\n",
       " '1y26_X': 4.281492799334667,\n",
       " '4lck_C': 12.413704737075747,\n",
       " '6pmo_A': 11.444848017756922,\n",
       " '5y85_B': 9.650666465387575}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
